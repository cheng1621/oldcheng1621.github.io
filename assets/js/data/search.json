[ { "title": "System Design", "url": "/posts/systemdesign/", "categories": "System Design", "tags": "System Design", "date": "2021-12-26 08:10:10 +0800", "snippet": "Requirement. (design intagram) users upload and download videos/photos/text user could follower other users. user could search the keyword. (return the phonts) system generate new feeds for different users.nonfunctional-requirement. system should be hightly available. the acceptable latency should be less than 200 ms. consistency could take a hit if user doesn’t see a photo for a while. reliable. any uploaded photo should never be lost.Design Consideration. the system should be read-heavy. (focus on the read operation) low-latency is expected viewing the photo. 100% relibalbecapacity estimation and constraints. 500M users.(total). 1M daily active users. 2M photos every day. 23 photos in a day. 150KB. (photo size)space: 150KB * 2M = 400G.for 10 years; 400GB * 365 * 10 = 1425 TB.API. getNewFeed(clientId,): feeds. Search(clientId,keywor): feeds. follow(clientId, DestinationID): bool upload(clientId, text, phote=None)high level Designflowchart TDA[client A] --&amp;gt; B(application server)E[client B] --&amp;gt; BB &amp;lt;--&amp;gt; C[(image database)]B &amp;lt;--&amp;gt; D[(metadata)]table.classDiagram class photo { photoId : int (primary key) userId: int photopath: string photosize: int CreationDate: datetime } class user { userId : int (primary key) follower: int name: string Email: string creationData: datetime } class userfolloer{ userId1 : int userId2 :int }data size estimation.useruserId(4bytes) + name(20 bytes) + email(32 bytes) === 68 bytes. 68 * 500M = 32 GBphoto0.5 GB per day" }, { "title": "Design Parking Lot", "url": "/posts/OOD_Design/", "categories": "OOD", "tags": "OOD", "date": "2021-12-21 19:10:10 +0800", "snippet": "Design Parking Lot.System Requirement. parking lot should have multiple floors, floors contain multiple spots. multiple entry and exit points. (implement later). customer get a ticket from entry point and return ticket at the exit point. payment can be in cash and via credit card.classDiagramclass Vehicletype { &amp;lt;&amp;lt;enumeration&amp;gt;&amp;gt; Car Truck Bike } class ParkingSpotType { &amp;lt;&amp;lt;enumeration&amp;gt;&amp;gt; Small Mediam Large XLarge } class AccountStatus { &amp;lt;&amp;lt;enumeration&amp;gt;&amp;gt; Active Closed Canceled }classDiagram %% ParkingLot, ParkingFloor and ParkingSpot. ParkingLot &quot;1&quot; o-- &quot;1..*&quot; ParkingFloor ParkingFloor &quot;1&quot; o-- &quot;1..*&quot; ParkingSpot class ParkingLot { Address: string id: int addEntryPoints() bool isFull() bool getTicket() Ticket addAttendant() : bool removeParkingRate() : bool scanTicket() : bool } class ParkingFloor { id: int isFull() bool } class ParkingSpot { id: int type: ParkingSpotType CarNumber: string Available: bool Free() bool } %% end %% entry points. (suppose there is one entry point) ParkingLot o-- Entrance ParkingLot o-- ExitPoint class Entrance { id: int print() void getTicket() Ticket } class ExitPoint{ id: int scanTicket() bool processPayment() bool } class Attendant { id : int scanTicket() bool processPayment() bool } %% Payment Payment &amp;lt;|-- CreditCardTransaction Payment &amp;lt;|-- CashTransaction class Payment { CreationDate : int Amount: double initialization(): bool } class CreditCardTransaction { name: string } class CashTransaction { } %% Account Account &amp;lt;|-- Admin Account &amp;lt;|-- Customer Account &amp;lt;|-- Attendant class Account { username : string password : string status : AccountStatus } class Admin { addAttendant() : bool } class Customer { getticket() : bool } class Attendant { processticket() : bool } %% Vehicle Vehicle &amp;lt;|-- Car Vehicle &amp;lt;|-- Truck Vehicle &amp;lt;|-- Bike class Vehicle { licenseNumber : string type : Vehicletype } class Car class Truck class Bike Design library.system requirement. customer can borrow a book from library return the book to the library. customer could buy book from libary. customer can search for keyword. (library should return the recommended book). Attendat can add or remove the new book to the system. customer could be notified if the book is available.class hierarchy.classDiagram %% Library. library o-- Panel library o-- Book class library { id : int address: string zipcode: int description : string search(string keyword) : Book Borrow(Book) : bool Return(Book) : bool Buy(book) : bool Add(Book) : bool Remove(book): book } class Panel { id : int search(string keyword) : Book Borrow(Book) : bool Return(Book) : bool Buy(book) : bool Add(Book) : bool Remove(book): book } %% Book class Book { id : int name: string author : string PublishedDate: datetime Location: string Status: BookStatus type: Booktype Amount: int } class BookStatus { &amp;lt;&amp;lt;enumeration&amp;gt;&amp;gt; Active Borrowed Removed } class Booktype { &amp;lt;&amp;lt;enumeration&amp;gt;&amp;gt; EconomicBook BusinessBook EntainmentBook } Book &amp;lt;|-- Bookitem %% class Bookitem { id : int barcode : string Status: BookStatus type : Booktype } %% BookLending. class BookLending { User: Account Bookitem: bookitem creationTime: datetime dueDate: datetime returnDate: dateTime. } %% Account library o-- Account Account &amp;lt;|-- Admin Account &amp;lt;|-- Member class Account { username: string password: string email: string id : int type: AccountType } class AccountType { &amp;lt;&amp;lt;enumeration&amp;gt;&amp;gt; Admin Customer Attendant } class Admin { addBook(): bool removeBook(): bool renewBook(): bool ReturnBook(): bool addMember(): bool } class Member { num_borrow : int borrowItem: booktime borrow() return() renewBook(): bool } %% fine transaction. class fine { amount: double getAmount() : double } fineTransaction &amp;lt;|-- CreditCardTransaction fineTransaction &amp;lt;|-- CashTransaction class fineTransaction { Amount : double intialization() : bool } class CreditCardTransction { } class CashTransction { }Design online Amazon shopping system.(OOD)requirement Member could buy items or return items. Member could add item to shopping card. Member could add comment to purchased items. Member could pay with credit card or debit card. Member could search for items by the name or catogory. Member could keep track of the shippment.class hierarchy.classDiagram %% Account class Address { &amp;lt;&amp;lt;data type&amp;gt;&amp;gt; name : string zipcode : int } class Account { username: string password: string email : string ShipmentAddress : Address mobile : string resetPassword() : bool } class Admin { addCatogory() } class Guest { search() : Product } class Member { search() : Product buy() : bool addToShoppingCard() : bool AddComment() : bool Shippment() returnItem() } Account &amp;lt;|-- Admin Account &amp;lt;|-- Guest Account &amp;lt;|-- Member %% Product. class Product { productId: int name : string rating: int description: string userId: int prices : double category: ProductCategory } class ProductCategory { &amp;lt;&amp;lt;enumeration&amp;gt;&amp;gt; Game Kid Food } %% used for search. Product o-- Catalog class Catalog { productName : map(name,list(product)) productCategory: map(ProductCategory,list(product)) } class item { amount: int prices: double updateAmount() : bool } item -- Product %% Shopping Card class ShoppingCard { items: list(items) print() : void removeItem() : bool AddItem() : bool } Member o-- ShoppingCard %% Order Order o-- OrderLog Member o-- Order class Order{ OrderId : string status : OrderStatus orderDate: datetime } %% OrderLog class OrderLog { creationDate: datetime status : OrderStatus } %% shippment class Shippment { ShippmentDate: date estimateArrival: date } %% shippment log class ShippmentLog { creationDate: datetime status : OrderStatus } Shippment o-- ShippmentLog Member o-- Shippment %% Payment class Payment { amount : double } class CreditCardPayment { intilization() } class DebitCardPayment { initialization() } Payment &amp;lt;|-- CreditCardPayment Payment &amp;lt;|-- DebitCardPayment %% Notification class Notification { notificationId : int content: string sendNotification() : bool } class EmailNotification { email : string } Notification &amp;lt;|-- EmailNotification Notification o-- Shippmentdesign stackOverflowRequirement Guests can search for questions. Users can post a question and post an answer to a question. (member) users and upvote for answer (member). member can add a comment on the comment section. member can tag the questions. System can identify most frequently used tags in the questions.##classDiagram %% actors. (Admin, guest, member, system) User &amp;lt;|-- Admin User &amp;lt;|-- Member class User { username: string password: string email: string } class Admin { closeQuestion(): bool addCatogery(): bool } class Member { PostQuestion(): bool PostAnswer(): bool AddComment(): bool UpVote() : bool } class Guest { search(); } %% Question &amp;amp; answer. class Question { title: string description: string Id: int viewCount : int voteCount : int creationDate : datetime status : QuestionStatus } class Answer { description: string accepted: bool votes: int createDate: Time }" }, { "title": "frangipani", "url": "/posts/Frangipani/", "categories": "frangipani", "tags": "frangipani", "date": "2021-12-06 19:10:10 +0800", "snippet": "IntroductionLogging and Recovery. when work station starts to update something on the file, write-ahead log is implemented in two places. first is the cache in the work station, and the other one is in the Petal. When work station1 crashes, but before releasing the lock (means the data after updated is still in work station1’s cache), another work station2 could see the log of WS1 and can replay the log. Version number could be used to replay the necessary log.synchronization and cache coherance. multiple-read/single write lock. Invalidate the data if read lock is asked to released. write dirty data into disk when write lock is asked to released.deadlock consists of two phases. 1st phase: acquire lock. 2nd phase: update the cache. (check the object which is related to 1st phase whether it is modified or not since acquiring the lock). if not. Commit. if it is modifiec, go back to phase 1.lock service.scenario: client could retain the lock until other client request it. client fails to renew the lease due to network failure (not crashes), file server should block all the subsequence requests and be unmounted to clear the error conditions.Implementation (schemes) one centralized lock service. (maybe cause performance problem) write the lock state to Petal (also causes performance problems) fully distributed. (Consists of Clerk Module). (asyncchronous message is better than RPC?)Adding or removing the servers.Easy, just shut it down when removing the servers, and tell the new server where is the Petal servers and lock server if adding the new servers.Some other Notes:avoid conflict of write and operation happens at the same timeVersion number" }, { "title": "Low-Latency Multi-Datacenter Databases using Replicated Commit Note", "url": "/posts/lowLatency_MultiDatacenter_Databases_Using_Replicated_Commit/", "categories": "Distributed System", "tags": "LowLatency", "date": "2021-11-26 08:10:10 +0800", "snippet": "Abstract.Instead of replicating the transactional log, we replicate the commit operation itself, by running Two-Phase Commit multiple timesin different datacenters and using Paxos to reach consensus among datacenters as to whether the transaction should commit.Motivaiton cross-datacenter one-way trip. inform the cohorts where the coordinator is. cross-datacenter roundtrip. Paxos leader needs to receive accepts from majority of replicas. cross-datacenter one-way trip cohorts inform the coordinator that the prepare phase is finished. cross-datacenter roundtrip. coordinator forward the commit log. cross-datacenter one-way trip coordinator sends commit message to cohorts. cross-datacenter roundtrip. cohorts receive accepts from majority of replicas.Replicated Commit all replicas of a data item are peers means there is no leader. Each server has its own lock table. Advantage: avoid the latency of leader election. usually, two consecutive election takes 10 s in Spanner. Disadvantage: Read Operations should send requests to all replicas and waits for response from all of them.Result. cross-datacenter one-way trip. clients send message to all coordinaters. 2PC only happens inside datacenter. cross-datacenter one-way trip. coordinator should communicate with each other and respond to client after 2PC is finished. coordinator sends commit to cohorts inside datacenter." }, { "title": "kafka Note", "url": "/posts/kafka_note/", "categories": "kafka", "tags": "MQ", "date": "2021-11-05 08:10:10 +0800", "snippet": "Introduction.Implemented in PacificA.DesignPersistence.pagecache-centric design and zero copy.constant time suffice.instead of using B-Tree, persistence queue is being built. O(1).Note: we can keep the message as long as we want.Efficiency. group the message together before sending it to the internet. zero copy.producer &amp;amp; consumer.poll for producer and pull for consumer.Message Delivery Semantics at-most-once. at-least-once. exactly-once.replication.a node is alive should satisfy two conditions. maintain session with zookeeper. replicate the write and not not fall “too far” behind.others.raft: to tolerate 1 failure, 3 replicas are needed.to solve this problem, ISR(in-sync replica) is needed.ISR.a write to partion is not considered as committed until all ISR replicas have received the write. and only the member of the set could be elected as leader.example: with f+1 replicas, kafka could tolerate f failures.Question. what if they all die.it a problem about tradeoff between durability and availability. wait. choose the first replica.Question. if we choose durability over availability. disable unclean leader election. specify a minimun ISR window. (it guarantees that the write can go to more than 1 replica and reduce the possibility that message is lost. )replica Management.avoid the situation that a high-volum partion is on a small amount of nodes.balance the leadership over each node.log compaction. take a snapshot for every key. delete old message.quota. network bandwidth. CPU utilization." }, { "title": "Amazon bq question", "url": "/posts/amazon_leadership/", "categories": "interview", "tags": "Amazon", "date": "2021-09-30 19:10:10 +0800", "snippet": "why Amazon?Amazon is one of the big tech company nowdays, and I believe working at Amazon is everyone’s dream. so am I. Besides of that, there are a lot of talented people in amazon creating amazing products to make people’s life easier. This spirit touch me a lot. I am always curious about the new stuff and I also have a great passion to learn something. when I heard about the word “distributed system”, I implemented a key/value storage system based on raft. I heard that facebook changed its name to meta, then I try to know what the metauniverse is. and I hope to work with someone who is also curious about everything and has a great passion to learn, and I think we can apply our idea into reality in the future to make the world better. that’s why I want to join amazon.why I choose this position.the first and the most important reason is that my experience matched the job description. the second reason that I like to create great services for customers to save their time and money. In my distributed system project, I care about low-latency and performance. When I implemented it. I implemented snapshot to improve the performance of log replaying. I divide the whole service into several partition to avoid the situation that one of the server becomes hot spot. I think in aws, there must be many ways to improve performance of the service. and I want to learn more about that. that’s what brings me here with this good opportunity.The biggest mistake you made and what did you learn from it? (earn trust, customer obsession.)Overview: miscalcuate the workload and working in CISDIS: When I was working as a software engineer in CISDI. My responsibility is to implement a web crawler. Sometimes my team member need a dataset from the internet and my responsiblity is to implement a web crawler and generate a dataset. T: I remembered that I got a task that I needed to implemented a web crawler to crawl a webpage in a week. A: My intentions to join the company was to learn new things and improve my programming skills. When I got this task. I tried to finish it in a perfect way. For example, I tried to choose multi-thread over single-thread. I implemented a filter which is used to filter the duplicate url out. I spent a log of time on searching for resources on the Internet. Then On Friday, After I finished the implementation, I ran the program on my local machine and started to generate the dataset. Until then, I noticed that the data is huge. Traffic Condition could be changed in a second. R: I just couldn’t finish it by the due date. I got a negative feedback from my boss. From that experience, I learned that I need to communicate with my mentor about my plan and have a good calculation about my task. and although the detais are really important, I should pay attention to the whole schedule to guarantee my schedule won’t affect others.TODO: TA job. or Research Job.Most challenging job.Overview: MIT 6.824S: My most challenging job is my distributed system project. T: It is challenging because I need to implement Raft consensus algorithm that is not easy to learn. I also need to implement a key/value service on top of raft. Besides that, I need to pay attention to some preformance, like performance of log replaying, and I shard the database to avoid being hot spot. Everything was new to me at that time. Besides that, I needed to learn a new language Go. When I came up with a new solution, I just tried it and started to implement. However, it’s always forces me to go back to the start point, just because I miscalculated how complex my idea was. Fortunately, I keeping trying and everything has been implemented. So through this experience, I learned that it’s important to be curious about something, the most important thing is to keeping trying. the more I tried, the closer I am to the success.couldn’t finish tasks before deadline. Related to biggest mistake.OverView: S: I still remembered that Last semester, I took a course about Compiler course. T: the professor usually propose the homework in advance. like I should finish it in 3 weeks. I was thinking, 3 weeks is too much. I keeping playing until one week bofore the due date. I suddenly received an interview invitaion in that week. which means i need to prepare for the interview while I need to finish the homework.R: I got rejected in that interview because I don’t have a good preparation. Although I finish my homework by the end of the due date. I got a pretty low score. From that experience, I learned that you should finish what I should do firstly, and thinking others. Since then, I start to make a priority list every two weeks. customer obsession.Overview: TA Job.S: Last semester, I worked as a TA in the database class . my responsibility is to grade the homework and hold the meeting and give a help to the student who has a problem.T: I still remembered that One student came to me, like please help me with the installation of a tool. he also sent me a snap about what the problem is.A1: I took a look at the snap, for me, the problem is really easy because I met this before. I googled some and send the link which I think is really helpful to him. I thought this thing is over. Next day, he came to me again, and sent me another snapshot. I took a look at the snap and found that the problem is almost the same. If he followed the instruction I sent to hom, it would work. The problem is that he didn’t follow the instruction and combine all the things together. Then I realised that I tried to help him in my way and didnt think in his way. That’s the main cause. Sometimes, some people just focus on how to use the tool and ignore how to install it. Then, I talked to him like we could meet in the library and figuare out the problem. Student need to go to school everyday. R: Student sends a good feedback to me, like I spend some of my personal time helping him. Ownership. Tell me about a time when you had to leave a task unfinished. S: When I worked as a software engineer in CISDI company. When the internship was close to end. T: I received a new job and I was told to finish it in two weeks. I realised that maybe the time is not enough for me to finish it. A: Then on the last two days, I just figure out what I did in this project and started to write a document, like the abstract of this project. where it’s now. What should be implemented more. What’s the plan of the furture. I looked into the code. Although there are comments, sometimes the comment. I just updated the comment.R: I hope it would be helpful for others who take over my job. tell me about a time when you had to work on a project with unclear responsibilities.S: When I worked as a software engineer in Schneider company. My boss hire two interns. Our reponsiblity is to implement Software. T: One day, My boss gave us a job which needs us to implement a small system.A: When I took that job. I talked to someone, figure out the requirments, like what does the interface look like. what features do I need to implement. My part went smoothly and I never meet a big difficulties. But for my coworker. Maybe the requiremnt is too much. maybe the feature he was going to add is really challenging. For some reasons, he stuck on something. When my part is finished and got a postive feedback from the member. I step in and decide to help him in my way. He passed me some requirements. during that time. I just talked to the custormer. I am taking over the job. I talked to the customer directly and make clear requirements. R: we got a postive feedback from my boss and customers. invent and simplify Tell me about a time when you gave a simple solution to a complex problem. Overview: project.When I implemented my distributed system project. Distributed system, the bug could happend anywhere at any time. it is really challenging to debug it. Then I ask myself, like why don’t I implement a small log filter. I just implemented it. I need to find where the problem happens. filter system just filter something out. are right, a lot." }, { "title": "gfs Note", "url": "/posts/gfsNote/", "categories": "Distributed System, MIT 6824", "tags": "Distributed System, gfs", "date": "2021-06-26 08:10:10 +0800", "snippet": "why.component failure is norm rather than exception.files are huge by traditional standards.design overview. suppose the component often fail. make optimation for multi-GB files. for random reads, batch and sort them to advance steadily through files rather than go back and forth. concurrent appendance by multiple clients to a same file is also supported.architecture.a master and several chunkservers.master maintains all the system metadata, including namespace,location … etc.neither the client nor the chunkserver caches file data, since the file maybe too large to be cached.Single master.process: client send packets consists of the file name and chunk index and the server responds with the locations of the chunk. Then client to one of the replicas for the data. no more client-master interactions are needed until the information expired.Chunk Size.usually 64 MB. advantages: reduce client-master interaction. Read and writes on the same file need one interaction. a client is more likely to perform many operations on the same files, it could reduce the network overhead. reduce the metadata which is stored in master.Metadata.three metadata: file namespace and chunk namespace, mapping file to chunk and locations of chunk&#39;s replicas.In-memory Data Structure.advantages: easy to scan through master’s state and implement the chunk garbage collection.chunk location. easy to request the data at startup. if the chunk is moved or failed, there is no point to maintain all the information.Operation Log.replicate operation log on mutiple remote machines and respond to a client operation after flushing the OL to disk both locally and remotely.Consistency Model.guarantee by GFS.some terminology: defined, consistent.GFS identifies failed chunkserver by checksumming.implication for applications.application mutates file by appending rather than overwriting. that means, a writer generates a file from begining to end.system interaction.master granted a lease to one of the replicas, named primary. primary define a mutation order. Sometimes primary could revoke the lease to disable the mutation on a file.1. client send requests to master for metadata of the chunk. 2. client push data to replicas.3. after the step 2, client send write request to primary.4. primary forward write request to secondary replica.5. secondary replies to primary.6. primary replies to client. data flow.to avoid network latency, one server send the information to closest server, instead of tree topology.atomic Record appends.GFS only guarantee the data is written at least once as an atomic unit, which means that data could include duplicate records.snapshot.copy on writemaster operation.namespace management and locking.GFS does not have a per-directory data structure that lists all the files in directory, nor does it support aliases.Replica Placement.Creation, Re-replication, reblancing.garbage collection.instead of reclaiming the delete object, the file changed to hidden object.stale replica detection.chunk version number is needed.fault tolerance and diagnosis.high availabilityfast recovery, chunk replication and master replication.if master is down, shadow master provides read-only access to the file system.data integrity.checksumming." }, { "title": "Computer Network Note", "url": "/posts/computer_network_note/", "categories": "Design Pattern", "tags": "Design Pattern", "date": "2021-06-05 19:10:10 +0800", "snippet": "Application layerHTTP.Overview.it uses TCP as underlying transport protocol. it maintains no information about the client.(stateless).non-persistent and persistent connections.each request or response is sent over a seperate connection vs … over the same TCP connections.(3 way handshake: 1. send a small TCP segment to server. 2. servers acknowledge the response. 3. client acknowledge back to the server)Message FormatGET: include data in the requested URL.POST: fill out a form.HEAD: response with HTTP message but leaves out the requested object.DELETE: delete the object on web servers.Some common status code.301: move permanently.400: bad request.(could not be understood by the server.)404: not found.505: HTTP version not supported.Cookies.used to identify users.web caching.it is also known as proxy server.(satisfies HTTP request on behalf of an origin web server.)two function: 1. reduce response time for request. 2. reduce the whole traffic in network.problem: if the object is stale. ans: conditional get(if-modified-since).electronic mailuser agent -&amp;gt; sender’s server -&amp;gt; receiver’s server -&amp;gt; receiver’s user agent.POP3.TCP connection. three phases: authorization, transaction, update.IMAP.create folders on remote servers.DNS.translate hostname to IP address. UDP protocol, port: 53.DASHthe video is encoded into several different versions. it allows clients with different internet access rate to stream in different encoding retes.CDN.suppose there’s one data center and client has to cross many communication links, resulting in delay for users.two service: determine a suitable CDN for clients and redirect client’s request to server.Transport Layer.provide communication services to application process and provides logical-communication between hosts.(IP makes its effort to deliver segments between communication hosts, but makes no guarantees.);Overview. difference between TCP and UDP.TCP provides reliable data transfer, congestion control. UDP: unreliable data transfer.multiplexing and demultiplexing.demultiplexing: deliver the data to socket.mutiplexing: group the data in different sockets and encapsulate each chunk with header information.UDPno handshaking, it is said to be connectionless.8 bytes overhead.UDP segments.4 parts,(each for 2 bytes). source port, destination port, length, checksum..Checksum.1’s complement.TCP.full-duplex.segment structure.header: 20 bytes.flow control.client maintain a receive window.3-way handshake.(problem SYN flooding) sends a segment encapsulated in IP datagram. (SYN = 1, Seq = random number); server allocates TCP buffers. send connection-granted segment back to client. allocate buffer and variable, but set SYN bit to zero.(can carry data.)client state:close -&amp;gt; syn_send -&amp;gt; established -&amp;gt; FIN_wait1 -&amp;gt; fin_wait2 -&amp;gt; time_wait -&amp;gt; close.server state:close -&amp;gt; listen -&amp;gt; SYN_received -&amp;gt; established -&amp;gt; close_wait -&amp;gt; last_ack -&amp;gt; close.problem1:if the server does not accept the segment on port 80 while the destination of the segment is port 80, server sends reset segment to host.(TCP);host would send ICMP datagram. (UDP);congestion control.slow start -&amp;gt; congestion avoidance -&amp;gt; fast recoverynetwork layerSwitching. switching via memory. it is seen as device I/O. switching via a bus. all output ports receive packets, but only one which matched the label can keep the packet. switching via an interconnection network. (overcome the bandwidth limitation.) capable of sending the packet in parallel.packet switching.FIFO, priority queue, Round Robin and Weighted Fair Queuing(WFQ)WFQ: suppose there are several priority queues. Scheduler check priority class one by one, if one class is empty, it check the next class immediately instead of waiting.IPv4.structure: 20 bytes.version number: 4 bits. header length, type of service.length of datagram: 16 bits, not bigger than 1500 bytes.offset : offset. time to live, checksum, source and destination IP and data.flag of last datagram is set to 0 while the others are set to 0;IPv4 addressing.subnet mask: means /24 in IP portion.exception: a host sends a datagram with IP 255.255.255.255 would be delivered to all hosts on the same subnet.DHCP: allocates different IP to a given host each time the host connects to the network. (UDP).NAT. (network address translation.)translate the local address to public address.Ipv6.datagram format.128 bits. header: 40 bytes. flow label. version, traffic class. next header, hop limit. source and destination address.fragmentation/reassembly: IPv6 does not allow fragmentation or reassembly at the intermediate routers. it can be performed only by the source and destination. IPv6 would send an ICMP error if the packet is too big.checksum and option.transition from IPv4 to IPv6sending host: take entire IPv6 datagram and put it in IPv4 datagram. Then the Ipv4 datagram addressed to Ipv6 node on the receiving side of tunnel.Generalized Forwarding and SDN.Control plane.per-router control: OSPF &amp;amp; BGP protocols.logically centralized control: controller computes and distributed forwarding tables to be used by each and every router.routing algorithm.centralized routing algorithm: be aware of the cost of each link in the network. (LS algorithm)decentralized routing algorithm: no node has complete information about costs of all nodes. (DS algorithm);LS algorithm.Dijkstra’s algorithm: computes the least-cost path from one node to all the other node in the network. (iterative);DV algorithm.a node x updates its distance-vector estimate when it either sees a cost change in one of its directly attached links or receives a distance-vector update from some neighbor.OSPF.(Intra-AS Routing in the Internet).two reasons: Scale and administrative autonomy.LS algorithm. Advances: Security. only trusted routers can participate in OSPF protocol within AS. Multiple same-cost paths. … BGP.all ASs run the same inter-AS routing protocol.two connections: eBGP(a BGP connection spans two ASs), and iBGP: routers in the same ASs.BGP attributes.AS-path: contain the list of ASs through which the advertisement has passed.NEXT-HOP: IP address of the router interface that begins the AS-PATH.destition prefix: 192.168.1.0/22. 1024 IPs.hot potato algorithmsend packet to others as soon as possible, not care which way it should go.Route-selection algorithm.note: if more than one routes arrive to one prefix, BGP sequentially invokes the following elimination rules until one route remains.IP-anycast.based on route-selection algorithm to pick the best path.SDN control plane.(???)OpenFlow protocol is implemented in controller layer.ICMP(internet control message protocol)it is used by hosts and routers to communicate network-layer information with each other.it is carried in IP datagram.note: ping sends ICMP type 8 code 0 message to host. traceroute also uses ICMP.network management. SNMP.application-layer protocol.framework managing server. managed device. MIB.Link layerservice.framing: it encapsulates network-layer datagram.link access: MAC protocol specifies the rules by which a frame is transmitted onto the link.reliable deliveryerror detection and correction.Error detection and correction.parity check0111000110101011 1checksuming methodsd bits of datagram is treated as k bit datagram ans sum this up to make a new checksum.CRCappend some bits to the end of bit stream, making it be divisible by G.Multiple Access links and protocol.channel partitioning protocol.avoid collision. TDM : limit rate of R/N, even there is one node in the network. a node has to wait for its turn for transmission. FDM: R/N, just as TDM.Random Access protocol.rate: R, wait a random delay before transmitting the frame. ALOHA and slot ALOHA.take-turns protocols. polling protocol. assign a node as master node (polling delay); tokenswitching local area networks.ARP: translate IP to MAC address.broadcast address: FF-FF-FF-FF-FF-FF.Ethernet.structure:CRC, data field, source destination address. no handshakeswitches.link-layer.features and properties:elimination of collisions &amp;amp; management.link virtualization.explain.id | selection type | table | partitions | type | keys | rows | filtered." }, { "title": "Design Pattern Note", "url": "/posts/design_pattern/", "categories": "Design Pattern", "tags": "Design Pattern", "date": "2021-06-05 19:10:10 +0800", "snippet": "Principle.Open Close principle. Liskov Substitution Principle. Dependence Inversion Principle. Interface Segregation Principle. Demeter Principle. Principle of single responsibility.Singleton.a class has only one instance which provide a global access pointer to this instance.Code.class singleton{private: string value; static singleton* singleton_;private: singleton(){};public: singleton* getinstance(){ if (singleton_ == NULL){ singleton_ = new singleton(); } return singleton_; }};singleton* singleton::singleton_ = nullptr;application.PC. task management. thread pool.factory mode.what is factory mode.code.// factory mode.class product {public: product(){}; virtual void show() = 0;};// product A.class productA : public product{public: productA(){}; void show(){ cout &amp;lt;&amp;lt; &quot;this is product A&quot; &amp;lt;&amp;lt; endl; }};// product B.class productB : public product{public: productB(){}; void show(){ cout &amp;lt;&amp;lt; &quot;this is product B&quot; &amp;lt;&amp;lt; endl; }};// product C.// factory.class factory{private: product* product_;public: factory(){} product* produce(string product){ if (product == &quot;productA&quot;){ this-&amp;gt;product_ = new ProductA(); } else { this-&amp;gt;product_ = new ProductB(); } return this-&amp;gt;product_; }};int main(){ factory* abstract_factory = new factory(); product* product; product = abstract_factory(&quot;productA&quot;); // product = abstract_factory(&quot;productB&quot;); return 0;}Strategy Pattern.what is strategy pattern.Example.class SortStrategy{public: virtual ~MapStrategy() {} virtual vector Doalgorithm(const vector&amp;lt;int&amp;gt; &amp;amp;data) const = 0;};class QuickSortStrategy : public SortStrategy{public: vector&amp;lt;int&amp;gt; Doalgorithm(const vector&amp;lt;int&amp;gt; &amp;amp; data){ vector&amp;lt;int&amp;gt; ans; // do quick sort algorithm. return ans; }};class MergeSortStrategy: public SortStrategy{public: vector&amp;lt;int&amp;gt; Doalgorithm(const vector&amp;lt;int&amp;gt; &amp;amp; data){ vector&amp;lt;int&amp;gt; ans; // do merge sort algorithm. return ans; }}// Context.class Context{private: SortStrategy *strategy;public: Context(SortStrategy* str1 = nullptr){ this-&amp;gt;strategy = str1; } ~Context(){ delete this-&amp;gt;strategy; } void setStrategy(SortStrategy *strategy){ delete this-&amp;gt;strategy; this-&amp;gt;startegy = strategy; } void display() const { cout &amp;lt;&amp;lt; }};int main(){ Context *context = new Context(new QuickSortStrategy); context-&amp;gt;display(); context-&amp;gt;setStrategy(new MergeSortStrategy); context-&amp;gt;display(); return 0;}Chain of responsibility.what is it?example.// Handler interface.class Handler{public: virtual bool handle(std::string request) = 0; virtual Handler* setnext(Handler* Handler) = 0;};// abstract handler.class AbstractHandler : public Handler{private: handler* next;public: AbstractHandler(){}; ~AbstractHandler(){}; bool handle(std::string request){ if (this-&amp;gt;next){ return this-&amp;gt;next-&amp;gt;handle(request); } return false; } Handler* setnext(Handler* handler){ this-&amp;gt;next = handler; return handler; } }// language handlerclass LanguageHandler : public AbstractHandler{public: bool handler(std::string request){ // logic handler. // 1. English. return 1. // 2. Chinese. // 3. Cantanese. return false; }};class DestinationHandler : public Handler{public: bool handler(std::string request){ // destination logic. return false; }};class TicketHandler : public Handler{public: bool handler(std::string request){ return false; }}Decorator// decoratorclass Component{public: virtual ~Component(){} virtual std::string Operation() const = 0;};class ConcreteComponent : public Component {public: std::string Operation() const override{ return &quot;concreteComponent&quot;; }};class Decorator : public Component{protected: Component* component_;public: Decorator(Component* c):component_(c){}; std::string Operation()const override{ return component_-&amp;gt;Operation(); }};class DecoratorA : public Decorator{public: DecoratorA(Component* c) : Decorator(c){}; std::string Operation() override{ return &quot;Decorator &quot; + Decorator::operation() &amp;lt;&amp;lt; endl; }};Command (Transaction)What is it.example.Abstract Factory.it is a creational design pattern that lets you produce families of related objects without specifying their concrete classes.class Shoes{public: virtual void shows() = 0; virtual ~Shoes(){}}class NikeShoes : public Shoes{public: void shows(){ cout &amp;lt;&amp;lt; &quot;this is nikeshoes&quot; &amp;lt;&amp;lt; endl; }};class clothes {public: virtual void shows() = 0; virtual ~clothes(){}};class uniClothes : public Clothes {public: void show(){ cout &amp;lt;&amp;lt; &quot;this is uniclothes &amp;lt;&amp;lt; endl; }}class AbstractFactory{public: virtual AbstractProduct_t *CreateProduct() = 0; virtual ~AbstractFactory() {}}class ConcreteFactory : public AbstractFactory{public: AbstractFactory *CreateProduct(){ return new ConcreteFactory() }}" }, { "title": "6.824 (假设能拿到offer的项目）", "url": "/posts/fault-tolerant-system/", "categories": "Distributed System", "tags": "Distributed System, Raft", "date": "2021-06-01 19:10:10 +0800", "snippet": "1.先来个自我介绍吧。嗯嗯，我之前写过一个基于LLVM的小型编译器，写过一个小型操作系统的kernel，最近写过一个有关分布式系统的，这个分布式系统实际上就是一个可容错性的分布式key/value数据库，支持动态扩容，负载均衡，线性一致性，这个一致性是基于raft共识算法，嗯，raft 就是为了在不同的服务器保持一致性的一种算法。这个数据库支持一定的容错，就是一个服务器崩了，另一个服务器马上就能取代原来的服务器开始工作。(说了那么多分布式你要是还问那个kernel项目，你真是头子)2.你为什么做那个分布式项目任何服务器都有可能在没有通知的情况下死机，并且一台服务器处理会造成超载，出现故障后可容错性和可恢复性差。所以分布式是非常有必要的。(不做的话难道等挂科？)3.那你简单介绍一下那个项目吧嗯，这个项目是一个key/value数据库，支持动态扩缩容,负载均衡,线性一致性, 可容错 和 可恢复。首先先说说负载均衡: 对数据库进行碎片化，通过对key 哈希取模确定虚拟碎片节点，再由虚拟服务器确定物理服务器。再说说动态扩缩容:假如说有10个虚拟节点，原来有3个物理节点，考虑负载均衡的话，那他们分别处理的虚拟节点数是3，4，3. 但是如果第四个物理节点加进来以后，那就是2，2，3，3，也就是第二个节点要分两个给新的物理节点。删除也是同样的道理。 增加和删除的操作靠一个controller来进行管理，controller本身也是一个raft集群。线性一致性:通过Raft共识算法实现线性一致性。例如： a:1 –&amp;gt; get(a) –&amp;gt; put(a : 2) –&amp;gt; get(a). 前一个get读到1， 后一个get读到2.可容错:每个节点都对应着由一组服务器，一个节点死机后由另一个服务器代替工作，服务器之间的一致性由raft管理。可恢复:快照 和 raft 日志回放。4.既然你提到raft,那你简单说一下raft的选举过程。（就猜到你问这个）嗯，raft 有三种状态，leader, candidate 和 follower. 如果follower超过过期时间没有接收到来自leader的心跳，那它就会变成candidate，同时发起选举，有两种情况它能获取选票。1. candidate的最后一个log的term 比 follower的 log term 要高。 2. term 相同的情况下，如果candidate 的 log 比follower 要长的话，它也能获得选票。 如果它获得大部人的选票的时候，它就变成了leader了，然后向其他的follower发送心跳。如果它没有收到足够的选票，他自己的term就会+1,然后发起新一轮的选票，或者他收到了来自leader的心跳重新变成follower.5.能说一下脑裂吗。脑裂就是假设有3个服务器，一开始都是follower的状态，但是他们基本同时变成了candidate，这种情况下是选不出leader的，一个简单的解决方法就是给每一个follower设置一个随机的过期时间。虽然同时变成candidate的这种情况依然存在，但是因为他们的过期时间是随机的，所以他们肯定在将来的某个term会选举出一个leader6.可容错和可恢复是怎么做的。可容错实际上就是raft，因为同一个集群中，他们始终能保持这一致性，一个服务器崩了，另一个服务器马上就选出新的leader开始新一轮的工作。可恢复就是，当log日志过大时，进行一个快照的操作，如果这个服务器重启了，就直接启用快照，然后log回放就可以了7.你这个负载均衡是怎么做的。负载均衡就是对client的请求hash取模，根据虚拟节点对物理节点的映射找到对应的物理节点。8.你提到hash取模，有没有听说过一致性哈希算法。 （还好我了解过）嗯，一致性哈希算法就是为了解决hash取模的局限性的，如果用hash取模，节点的增加或者减少会导致大量映射的改变，而一致性哈希算法就是为了尽可能少地改变映射关系。9.那你这个项目有什么难点吗难点吧，有几个 确保各种可靠性。首先在扩容的时候，因为节点的增加要把旧节点的数据同步到新节点，并停止旧节点的服务，假设一种情况：一个client发送append请求后，节点才停止服务，但此时append请求已经到达raft状态机等待commit, 正常来说这个put请求就是无效的，所以就算是commit过了也不能更改k/v数据库中的消息，我的办法就是在commit过后再判断一次节点的服务是否在线，如果不在线就不要修改了。在raft 选举中也是，如果candidate 得到了大量的选票， 但此时他已经不是candidate 的状态了，此时就算是获得了足够的选票也不能变为leader，因为从Candidate 变为 follower说明集群中的某个服务器的term 肯定比自己高。leader发送心跳的时候也是，发送之前和发送之后如果发生了状态的改变，也要当前的更改操作。在这个项目中，需要大量的状态检测来确保每一个操作都是可靠的，因为无法控制哪个goroutine先被运行。 分区中，例如一个leader死机了一段时间又重新加入到集群中，这个leader就会向其他的集群发送心跳和日志复制等消息，但显然这些都是无效的信息，follower不能对心跳作出反应，当然这个就比较好解决了，因为死机后那个leader的term一定比新leader的term低，可以根据这个term来加以判断。 10.听你这么说也不是很难啊。（哥，我感觉真的挺难）11.那你又做什么优化吗。 扩容的时候，配置更新的时候，一开始我的做法就是锁住整个服务，等到配置更新完毕后再释放，如一开始，group1 负责 分区1，分区2，分区3,但是新的配置是将 分区1 的数据转移到 group2, 这时候就没必要锁住整个服务了，分区2 和 分区3 还是能正常接收client的请求的。 配置的时候，尽可能少得改变当前配置，如前面所说3,4,3 -&amp;gt; 2,2,3,3 直接从4中划分两个节点出来就可以了，没必要重新洗牌再分配。 转移分区的后，把旧节点中停止服务的分区数据清理掉，避免造成空间的浪费。 对 client 重复的请求直接返回数据。下面的两个是raft 状态机的优化。 假设一台服务器死机的时间过长，当它重新启动的时候，它会接收snapshot 和 log 回放，假设如下情况：第一张图的情况是当一个服务器重启后再连上的时候发生的，根据raft，leader每次都把log递减，就是： 1st. log[] index:1000 2nd. log[6] index:999 3rd. log[6,6]index:998. 直到找到发生冲突的第一个log，在这种情况下就需要发送995次心跳。 第二张图是当raft集群是在网络不稳定的产生，raft内部的leader不断变换。 在这种情况下也需要8次的心跳。优化：第一张图：当follower接收到leader的心跳后，因为follower不存在index为1000的log， 所以直接返回自己的log 长度，告诉leader从这里开始发送，在这种情况下，心跳的次数变成了3次。第二张图：term:6 != term :3. follower 直接返回term 为3 的最左边的序号，心跳的次数为：3.这个优化是在尽可能少地减少网络请求的情况下找到产生冲突的那个log. 一开始日志的复制完全依赖于心跳，而心跳的速度不宜过快，大概为 200ms 一次，但是就算是上面的那两种情况下也需要 600ms 才能完成日志的复制。优化：当接收到来自client的请求后马上发送心跳，同时设置心跳过期时间，避免重发。这样的结果就是日志的同步更快了，缺点就是网络请求的增加。导致的问题就是：假设raft 分别接收到两个请求：request1 和 request 2. 但是假如request2 比request1 先处理，不加状态检测的话，就会导致刚加进来的log意外地丢失，所以还是要在操作之前判断这种情况是否发生。 优化结果就是：一开始一个请求平均需要 120ms -&amp;gt; 到9ms. (强不强。!_!)12.End." }, { "title": "6.824", "url": "/posts/fault-system-system-v2/", "categories": "Distributed System", "tags": "Distributed System, Raft", "date": "2021-05-29 09:19:10 +0800", "snippet": "情景：任何服务器都有可能在没有通知的情况下死机，并且一台服务器处理会造成超载，出现故障后可容错性和可恢复性差。任务：实现一个K/V集群，支持动态扩缩容,负载均衡,线性一致性, 可容错 和 可恢复。流程：client 请求 Shard controller 获取当前的配置，把请求发到对应的物理节点，物理节点把请求放到对应的raft状态机中，直到状态机commit后返回结果给client。基本架构。负载均衡:对数据库进行碎片化，通过对key 哈希取模确定虚拟碎片节点，再由虚拟服务器确定物理服务器。动态扩缩容:用单独的一个raft集群 (Shard controller) 管理动态扩缩容。Shard controller作用: 处理client发送的请求, 根据请求找到处理请求的物理节点。 controller 帮助 物理节点找到它所负责的那一部分虚拟节点。线性一致性:通过Raft共识算法实现线性一致性。a:1 –&amp;gt; get(a) –&amp;gt; put(a : 2) –&amp;gt; get(a). 前一个get读到1， 后一个get读到2.可容错:每个节点都对应着由一组服务器，一个节点死机后由另一个服务器代替工作，服务器之间的一致性由raft管理。可恢复:snapshot快照 和 raft 日志回放。实现Shard controller.支持的请求类型： Query(n). 获取特定的配置。 join(group) 增加物理节点。 leave(group) 删除物理节点 move : 改变虚拟节点对物理节点的映射。 type ShardCtrler struct { mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg // Your data here. configs []Config // indexed by config num scClient map[int64]int64 // map clientId to SeqId // need channel LastCommitIndex int32 dead int32} Shard controller 保存 配置信息（configs)，client 可以通过请求配置 得到 物理节点。物理节点通过请求配置得到要处理的虚拟节点。 type Config struct { Num int // config number Shards [NShards]int // shard -&amp;gt; gid Groups map[int][]string // gid -&amp;gt; servers[]} Shards: 物理节点对虚拟节点的映射。Groups: 物理节点对服务器的映射。 难点：发生状态改变的时候尽可能地减少映射。物理节点。支持的请求类型： get: 获取对应的值 append/put: 修改值。 type ShardKV struct { mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg make_end func(string) *labrpc.ClientEnd gid int ctrlers []*labrpc.ClientEnd maxraftstate int // snapshot if log grows this big kvShard []Shard LastCommitIndex int32 persister *raft.Persister // raft Persister. dead int32 // set by Kill() config shardctrler.Config sm *shardctrler.Clerk service []int} 每个物理节点维持一个Shard结构体,储存着对应的虚拟节点的数据。 保留着当前的配置（config). 和供可恢复的persister. type Shard struct{ ShardId int KvDB map[string]string KvClient map[int64]int64 Version int} 思考：配置发生改变的时候，也就是说把虚拟节点的数据移到新的物理节点，假设检测到配置的改变和接收到client的get请求同时发生，是等待get 请求完成还是转移后再处理get?难点：如何在配置发生改变的同时仍能保证一致性和程序的正常运行。 raft状态机支持的功能： 可用性。 5个服务器可忍受2个服务器死机 一致性： 各个raft之间要实现数据的同步。 分区。 5个服务器分成2/3。 分区一段时间后，合并后仍能恢复一致性。实现每一个服务器对应着一个raft状态机。每个raft状态机有三个状态，leader, candidate, follower.选举：follower没有接收到来自leader的心跳后变成candidate发起投票。两种方式能获取选票， candidate’s log term 比 follower‘s log term 要高。 term 相同的情况下，candidate’s log 比 follower’s log 要长。获取超过一半选票的时候就变成leader，然后发送心跳或者日志的复制。难点：分区后合并仍能保证一致性，并注重各个协程中的同步过程。优化： 假设一台服务器死机的时间过长，当它重新启动的时候，它会接收snapshot 和 log 回放，假设如下情况：正常情况下，leader首先发送一个空的日志给follower，起点在1000，因为follower 不存在标号为1000 的日志，它就会返回一个错误值。然后leader对日志标号减一，这个时候会发送一个长度为1的日志，标号为999，但是follower 不存在，返回错误值。…直到日志首次发生错误的地方开始复制。根据raft所说，要找到他们首次出现不同的地方开始复制，在上面leader 需要发送RPC 给 follower，" }, { "title": "6.824 lab2D", "url": "/posts/6824-lab2D/", "categories": "Distributed System", "tags": "Distributed System, Raft", "date": "2021-04-22 19:10:10 +0800", "snippet": "SummaryLog in Raft cannot grow without bound, since long Log occupies space and takes time to replay. Snapshotting is a simplest approach to compaction.Some explanations. every servers take snapshot independently. an indexing scheme independent of log position is required, as snapshot would trim log sometimes. In my implementation, I add two parameters into Raft structure, named with LastIncludedIndex and LastIncludedTerm. I combine sendInstallsnapshot with heartbeat function and if rf.nextIndex[index] &amp;lt; rf.LastIncludedIndex, send snapshot. Modifications to lines are needed where I use len(rf.Log) to represent the index of the Log.Result.Code.package raft//// this is an outline of the API that raft must expose to// the service (or tester). see comments below for// each of these functions for more details.//// rf = Make(...)// create a new Raft server.// rf.Start(command interface{}) (index, term, isleader)// start agreement on a new log entry// rf.GetState() (term, isLeader)// ask a Raft for its current term, and whether it thinks it is leader// ApplyMsg// each time a new entry is committed to the log, each Raft peer// should send an ApplyMsg to the service (or tester)// in the same server.//import ( &quot;bytes&quot; &quot;sync&quot; &quot;sync/atomic&quot; &quot;6.824/labgob&quot; &quot;6.824/labrpc&quot;)// new import.import &quot;time&quot;import &quot;math/rand&quot;// 3 states for servers.const Candidate = 1const Follower = 2const Leader = 3//// as each Raft peer becomes aware that successive log entries are// committed, the peer should send an ApplyMsg to the service (or// tester) on the same server, via the applyCh passed to Make(). set// CommandValid to true to indicate that the ApplyMsg contains a newly// committed log entry.//// in part 2D you&#39;ll want to send other kinds of messages (e.g.,// snapshots) on the applyCh, but set CommandValid to false for these// other uses.//type ApplyMsg struct { CommandValid bool Command interface{} CommandIndex int // For 2D: SnapshotValid bool Snapshot []byte SnapshotTerm int SnapshotIndex int}//// A Go object implementing a single Raft peer.//type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer&#39;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer&#39;s persisted state me int // this peer&#39;s index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). // Look at the paper&#39;s Figure 2 for a description of what // state a Raft server must maintain. // (new). state int Time time.Time VoteCount int Timeout int // (new) end CurrentTerm int VoteFor int Log []LogEntry commitIndex int lastApplied int nextIndex []int matchIndex []int applyCh chan ApplyMsg LastIncludedIndex int LastIncludedTerm int}// return currentTerm and whether this server// believes it is the leader.func (rf *Raft) GetState() (int, bool) { var term int var isleader bool // Your code here (2A). rf.mu.Lock() defer rf.mu.Unlock() term = rf.CurrentTerm if rf.state == Leader { isleader = true } return term, isleader}//// save Raft&#39;s persistent state to stable storage,// where it can later be retrieved after a crash and restart.// see paper&#39;s Figure 2 for a description of what should be persistent.//func (rf *Raft) persist() { // Your code here (2C). // Example: w := new(bytes.Buffer) e := labgob.NewEncoder(w) e.Encode(rf.CurrentTerm) e.Encode(rf.VoteFor) e.Encode(rf.Log) data := w.Bytes() rf.persister.SaveRaftState(data)}//// restore previously persisted state.//func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) &amp;lt; 1 { // bootstrap without any state? return } // Your code here (2C). // Example: r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var term int var votefor int var log []LogEntry if d.Decode(&amp;amp;term) != nil || d.Decode(&amp;amp;votefor) != nil || d.Decode(&amp;amp;log) != nil { return } else { rf.CurrentTerm = term rf.VoteFor = votefor rf.Log = log }}// snapshot argument.type SnapshotArgs struct { Term int LeaderId int LastIncludedIndex int LastIncludedTerm int Data []byte Done bool // todo. Offset.}// snapshot resulttype SnapshotReply struct { Term int LastIncludedIndex int}// the service says it has created a snapshot that has// all info up to and including index. this means the// service no longer needs the log through (and including)// that index. Raft should now trim its log as much as possible.func (rf *Raft) Snapshot(index int, snapshot []byte) { // Your code here (2D). rf.mu.Lock() defer rf.mu.Unlock() i := 0 n := len(rf.Log) for ;i &amp;lt; n;i++ { if rf.LastIncludedIndex + i &amp;gt;= index { break } rf.LastIncludedTerm = rf.Log[i].Term } // update lastincludedindex. rf.LastIncludedIndex = rf.LastIncludedIndex + i; if i == n { // trim the entire log. rf.Log = rf.Log[:0] rf.lastApplied = max(rf.LastIncludedIndex,rf.lastApplied) rf.commitIndex = max(rf.LastIncludedIndex,rf.commitIndex) } else { rf.Log = rf.Log[i:] rf.lastApplied = max(rf.LastIncludedIndex,rf.lastApplied) rf.commitIndex = max(rf.LastIncludedIndex,rf.commitIndex) } // update the data. rf.persist() rf.persister.SaveStateAndSnapshot(rf.persister.ReadRaftState(),snapshot) return}// send InstallSnapshot RPC.func (rf *Raft) sendInstallSnapshot(server int,args *SnapshotArgs, reply *SnapshotReply) bool { ok := rf.peers[server].Call(&quot;Raft.InstallSnapshotHandler&quot;, args, reply) return ok}// InstallSnapshot handler.func (rf *Raft) InstallSnapshotHandler(args *SnapshotArgs, reply *SnapshotReply) { rf.mu.Lock() reply.Term = rf.CurrentTerm DPrintf(&quot;me:%d, lastIncludedIndex: %d&quot;,rf.me,rf.LastIncludedIndex) reply.LastIncludedIndex = rf.LastIncludedIndex if rf.CurrentTerm &amp;gt; args.Term { rf.mu.Unlock() return } snapshot := clone(args.Data) msg := ApplyMsg { CommandValid : false, // For 2D: SnapshotValid : true, Snapshot : snapshot, SnapshotTerm : args.LastIncludedTerm, SnapshotIndex : args.LastIncludedIndex, } rf.mu.Unlock() rf.applyCh &amp;lt;- msg}//// A service wants to switch to snapshot. Only do so if Raft hasn&#39;t// have more recent info since it communicate the snapshot on applyCh.//func (rf *Raft) CondInstallSnapshot(lastIncludedTerm int, lastIncludedIndex int, snapshot []byte) bool { // Your code here (2D). rf.mu.Lock() defer rf.mu.Unlock() if lastIncludedTerm &amp;lt; rf.LastIncludedTerm || lastIncludedIndex &amp;lt; rf.LastIncludedIndex{ return false } i := 0 n := len(rf.Log) for ;i &amp;lt; n;i++ { if rf.LastIncludedIndex + i &amp;gt;= lastIncludedIndex { break } rf.LastIncludedTerm = lastIncludedTerm } rf.LastIncludedIndex = lastIncludedIndex if i == n { // trim the entire log. rf.Log = rf.Log[:0] rf.lastApplied = max(rf.LastIncludedIndex,rf.lastApplied) rf.commitIndex = max(rf.LastIncludedIndex,rf.commitIndex) } else { rf.Log = rf.Log[i:] rf.lastApplied = max(rf.LastIncludedIndex,rf.lastApplied) rf.commitIndex = max(rf.LastIncludedIndex,rf.commitIndex) } rf.persist() rf.persister.SaveStateAndSnapshot(rf.persister.ReadRaftState(),snapshot) return true}// (2A) structure log entries.type LogEntry struct { Term int Command interface{} // still don&#39;t know about the command.}// (2A) AppendEntries structure.type AppendEntriesArgs struct { Term int Leaderindex int PrevLogIndex int PrevLogTerm int Entries []LogEntry LeaderCommit int}type AppendEntriesReply struct { Term int Success bool // (2C) new parameter. Xterm int Xindex int}// (2A) AppendEntries Handler.func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply){ rf.mu.Lock() defer rf.mu.Unlock() DPrintf(&quot;me:%d,term: %d,lastApplied:%d,commitIndex:%d,lastIncludedIndex:%d.&quot;,rf.me,rf.CurrentTerm,rf.lastApplied,rf.commitIndex,rf.LastIncludedIndex) DPrintf(&quot;me:%d, %v&quot;,rf.me,rf.Log) if args.Term &amp;lt; rf.CurrentTerm { reply.Term = rf.CurrentTerm reply.Success = false return } // (2B.) // (2B).Figure 2: AppendEntries RPC, implementation 2. something if args.Term &amp;gt; rf.CurrentTerm { rf.CurrentTerm = args.Term rf.persist() rf.convertToFollower() } if len(rf.Log) + rf.LastIncludedIndex &amp;lt; args.PrevLogIndex { reply.Term = rf.CurrentTerm reply.Success = false reply.Xindex = len(rf.Log) + rf.LastIncludedIndex reply.Xterm = -1 rf.Time = time.Now() return } if args.PrevLogIndex &amp;gt; rf.LastIncludedIndex &amp;amp;&amp;amp; args.PrevLogIndex &amp;lt; len(rf.Log) + 1 + rf.LastIncludedIndex &amp;amp;&amp;amp; rf.Log[args.PrevLogIndex - 1 - rf.LastIncludedIndex].Term != args.PrevLogTerm { reply.Term = rf.CurrentTerm reply.Success = false reply.Xterm = rf.Log[args.PrevLogIndex - rf.LastIncludedIndex - 1].Term for i := 0;i &amp;lt; len(rf.Log);i++{ if rf.Log[i].Term == reply.Xterm { reply.Xindex = i + 1 + rf.LastIncludedIndex break } } rf.Time = time.Now() return } // implementation 3 &amp;amp; implementation 4. delete existing entry if conflict happens index := -1 for i := 0;i &amp;lt; len(args.Entries);i++{ if len(rf.Log) + rf.LastIncludedIndex &amp;lt; (args.PrevLogIndex + 2 + i) || rf.Log[args.PrevLogIndex + 1 + i - rf.LastIncludedIndex].Term != args.Entries[i].Term{ index = i break } } if index != -1 { rf.Log = rf.Log[:index + args.PrevLogIndex - rf.LastIncludedIndex] for i := index;i &amp;lt; len(args.Entries);i++{ rf.Log = append(rf.Log,args.Entries[i]) } rf.persist() } // implementation 5. if args.LeaderCommit &amp;gt; rf.commitIndex { rf.Commit(min(args.LeaderCommit,len(rf.Log) + rf.LastIncludedIndex)) } reply.Term = rf.CurrentTerm reply.Success = true rf.Time = time.Now()}func (rf *Raft) sendAppendEntries(server int,args *AppendEntriesArgs, reply *AppendEntriesReply) bool { ok := rf.peers[server].Call(&quot;Raft.AppendEntries&quot;, args, reply) return ok}//// example RequestVote RPC arguments structure.// field names must start with capital letters!//type RequestVoteArgs struct { // Your data here (2A, 2B). Term int Candidateindex int LastLogIndex int LastLogTerm int}//// example RequestVote RPC reply structure.// field names must start with capital letters!//type RequestVoteReply struct { // Your data here (2A). Term int VoteGranted bool}//// example RequestVote RPC handler.//func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { // Your code here (2A, 2B). rf.mu.Lock() defer rf.mu.Unlock() if args.Term &amp;lt; rf.CurrentTerm { reply.Term = rf.CurrentTerm reply.VoteGranted = false; return } if args.Term &amp;gt; rf.CurrentTerm { rf.CurrentTerm = args.Term rf.persist() rf.convertToFollower() } if rf.VoteFor == -1 || rf.VoteFor == args.Candidateindex { // (2B. leader election.) if len(rf.Log) &amp;gt; 0 { if args.LastLogTerm &amp;gt; rf.Log[len(rf.Log) - 1].Term { rf.VoteFor = args.Candidateindex reply.VoteGranted = true reply.Term = rf.CurrentTerm rf.Time = time.Now() rf.persist() return } if (args.LastLogTerm == rf.Log[len(rf.Log) - 1].Term &amp;amp;&amp;amp; args.LastLogIndex &amp;gt;= len(rf.Log) + rf.LastIncludedIndex){ rf.VoteFor = args.Candidateindex reply.VoteGranted = true reply.Term = rf.CurrentTerm rf.Time = time.Now() rf.persist() return } } else { if args.LastLogTerm &amp;gt; rf.LastIncludedTerm || (args.LastLogTerm == rf.LastIncludedTerm &amp;amp;&amp;amp; args.LastLogIndex &amp;gt;= rf.LastIncludedIndex){ rf.VoteFor = args.Candidateindex reply.VoteGranted = true reply.Term = rf.CurrentTerm rf.Time = time.Now() rf.persist() return } } } reply.VoteGranted = false reply.Term = rf.CurrentTerm}//// example code to send a RequestVote RPC to a server.// server is the index of the target server in rf.peers[].// expects RPC arguments in args.// fills in *reply with RPC reply, so caller should// pass &amp;amp;reply.// the types of the args and reply passed to Call() must be// the same as the types of the arguments declared in the// handler function (including whether they are pointers).//// The labrpc package simulates a lossy network, in which servers// may be unreachable, and in which requests and replies may be lost.// Call() sends a request and waits for a reply. If a reply arrives// within a timeout interval, Call() returns true; otherwise// Call() returns false. Thus Call() may not return for a while.// A false return can be caused by a dead server, a live server that// can&#39;t be reached, a lost request, or a lost reply.//// Call() is guaranteed to return (perhaps after a delay) *except* if the// handler function on the server side does not return. Thus there// is no need to implement your own timeouts around Call().//// look at the comments in ../labrpc/labrpc.go for more details.//// if you&#39;re having trouble getting RPC to work, check that you&#39;ve// capitalized all field names in structs passed over RPC, and// that the caller passes the address of the reply struct with &amp;amp;, not// the struct itself.//func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) bool { ok := rf.peers[server].Call(&quot;Raft.RequestVote&quot;, args, reply) return ok}//// the service using Raft (e.g. a k/v server) wants to start// agreement on the next command to be appended to Raft&#39;s log. if this// server isn&#39;t the leader, returns false. otherwise start the// agreement and return immediately. there is no guarantee that this// command will ever be committed to the Raft log, since the leader// may fail or lose an election. even if the Raft instance has been killed,// this function should return gracefully.//// the first return value is the index that the command will appear at// if it&#39;s ever committed. the second return value is the current// term. the third return value is true if this server believes it is// the leader.//func (rf *Raft) Start(command interface{}) (int, int, bool) { index := -1 term := -1 isLeader := true // Your code here (2B). rf.mu.Lock() defer rf.mu.Unlock() if rf.state != Leader { isLeader = false return index, term, isLeader } // append command to rf.Log. new_log := LogEntry{} new_log.Term = rf.CurrentTerm new_log.Command = command rf.Log = append(rf.Log,new_log) // append new command to local log. rf.persist() index = len(rf.Log) + rf.LastIncludedIndex term = rf.CurrentTerm rf.matchIndex[rf.me] = index rf.nextIndex[rf.me] = index + 1 return index, term, isLeader}//// the tester doesn&#39;t halt goroutines created by Raft after each test,// but it does call the Kill() method. your code can use killed() to// check whether Kill() has been called. the use of atomic avoids the// need for a lock.//// the issue is that long-running goroutines use memory and may chew// up CPU time, perhaps causing later tests to fail and generating// confusing debug output. any goroutine with a long-running loop// should call killed() to check whether it should stop.//func (rf *Raft) Kill() { atomic.StoreInt32(&amp;amp;rf.dead, 1) // Your code here, if desired.}func (rf *Raft) killed() bool { z := atomic.LoadInt32(&amp;amp;rf.dead) return z == 1}// The ticker go routine starts a new election if this peer hasn&#39;t received// heartsbeats recently.func (rf *Raft) ticker() { for rf.killed() == false { // Your code here to check if a leader election should // be started and to randomize sleeping time using // time.Sleep(). rf.mu.Lock() go rf.SendCommitEntry() switch rf.state { case Follower: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout{ rf.converttoCandidate() } rf.mu.Unlock() case Candidate: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout { go rf.getVote() } rf.mu.Unlock() case Leader: rf.heartbeat() rf.mu.Unlock() } time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond) }}func (rf *Raft) convertToLeader() { // (2B.) // initialization. (nextIndex and matchIndex.) rf.nextIndex = make([]int,len(rf.peers)) rf.matchIndex = make([]int,len(rf.peers)) for i:= 0;i &amp;lt; len(rf.peers);i++{ rf.nextIndex[i] = len(rf.Log) + rf.LastIncludedIndex + 1 rf.matchIndex[i] = 0 } rf.state = Leader go rf.heartbeat()}func (rf *Raft) converttoCandidate(){ rf.CurrentTerm++ rf.VoteFor = rf.me rf.state = Candidate rf.VoteCount = 1; rf.persist() go rf.getVote()}// send to other server except the Candidate server.func (rf *Raft) getVote(){ rf.mu.Lock() rf.CurrentTerm++ rf.Timeout = rand.Intn(200) + 400 rf.Time = time.Now() rf.VoteCount = 1 rf.persist() rf.mu.Unlock() // (2B. log replication) for i := 0;i &amp;lt; len(rf.peers);i++{ if i != rf.me { go func(index int){ rf.mu.Lock() args := RequestVoteArgs{ Term: rf.CurrentTerm, Candidateindex : rf.me, LastLogIndex: len(rf.Log) + rf.LastIncludedIndex, LastLogTerm: rf.LastIncludedTerm, } if len(rf.Log) &amp;gt; 0 { args.LastLogTerm = rf.Log[len(rf.Log) - 1].Term } rf.mu.Unlock() reply := RequestVoteReply{} ok := rf.sendRequestVote(index,&amp;amp;args,&amp;amp;reply) if !ok { return } rf.mu.Lock() defer rf.mu.Unlock() if rf.state != Candidate { return } if reply.VoteGranted { rf.VoteCount++ } if rf.VoteCount &amp;gt; len(rf.peers) / 2{ rf.convertToLeader() return } }(i) } }}func (rf *Raft) convertToFollower(){ rf.state = Follower rf.VoteFor = -1 rf.persist()}// heartbeat. empty AppendEntries to keep authority.func (rf *Raft) heartbeat(){ for i := 0;i &amp;lt; len(rf.peers);i++{ if i != rf.me{ go func(index int) { rf.mu.Lock() if rf.state != Leader { rf.mu.Unlock() return } DPrintf(&quot;me:%d,%v&quot;,rf.me,rf.Log) DPrintf(&quot;nextIndex[%d] : %d&quot;,index,rf.nextIndex[index]) // send snapshot if log lag behind. if rf.LastIncludedIndex &amp;gt; 0 &amp;amp;&amp;amp; rf.nextIndex[index] &amp;lt;= rf.LastIncludedIndex { args := SnapshotArgs { Term : rf.CurrentTerm, LeaderId : rf.me, LastIncludedIndex : rf.LastIncludedIndex, LastIncludedTerm : rf.LastIncludedTerm, Data : rf.persister.ReadSnapshot(), } reply := SnapshotReply{} rf.mu.Unlock() ok := rf.sendInstallSnapshot(index,&amp;amp;args,&amp;amp;reply) if ok { rf.mu.Lock() if reply.Term &amp;gt; rf.CurrentTerm { rf.CurrentTerm = reply.Term rf.convertToFollower() } else { rf.nextIndex[index] = reply.LastIncludedIndex + 1 rf.matchIndex[index] = reply.LastIncludedIndex } rf.mu.Unlock() } return } prevLogIndex := rf.nextIndex[index] - 1 entries := make([]LogEntry, len(rf.Log[rf.nextIndex[index] - 1 - rf.LastIncludedIndex:])) copy(entries, rf.Log[(prevLogIndex - rf.LastIncludedIndex):]) args := AppendEntriesArgs{ Term : rf.CurrentTerm, Leaderindex : rf.me, PrevLogIndex :prevLogIndex, PrevLogTerm : rf.LastIncludedTerm, // Entries : rf.Log[PrevLogIndex + 1:], Entries: entries, LeaderCommit : rf.commitIndex, } if prevLogIndex &amp;gt; rf.LastIncludedIndex { args.PrevLogTerm = rf.Log[prevLogIndex - 1 - rf.LastIncludedIndex].Term } if rf.state != Leader { rf.mu.Unlock() return } rf.mu.Unlock() reply := AppendEntriesReply{} ok := rf.sendAppendEntries(index,&amp;amp;args,&amp;amp;reply) if !ok { return } rf.mu.Lock() defer rf.mu.Unlock() if reply.Success { rf.matchIndex[index] = args.PrevLogIndex + len(args.Entries) rf.nextIndex[index] = rf.matchIndex[index] + 1 // check if replicated on majority of servers. for N := len(rf.Log) + rf.LastIncludedIndex; N &amp;gt; rf.commitIndex;N--{ num := 0 for j:= 0;j &amp;lt; len(rf.peers);j++{ if rf.matchIndex[j] &amp;gt;= N{ num++; } if num &amp;gt; len(rf.peers) / 2 { rf.Commit(N) break } } } } else { if reply.Term &amp;gt; rf.CurrentTerm { rf.CurrentTerm = reply.Term rf.persist() rf.convertToFollower() } else { // (2C.) if reply.Xindex &amp;gt; 0 { firstConflict := reply.Xindex if reply.Xterm != -1 { // not missing logs for i := 0; i &amp;lt; len(rf.Log); i++ { if rf.Log[i].Term != reply.Xterm { continue } for i &amp;lt; len(rf.Log) + 1 &amp;amp;&amp;amp; rf.Log[i].Term == reply.Xterm { i++ // the last conflict log&#39;s next index } firstConflict = i break } } rf.nextIndex[index] = firstConflict // next sync, send conflicted logs to the follower } else { rf.nextIndex[index] = 1 } // if reply.Term &amp;gt; rf.CurrentTerm { // rf.CurrentTerm = reply.Term // rf.convertToFollower() // } else { // if rf.nextIndex[index] &amp;gt; 1 { // rf.nextIndex[index] = args.PrevLogIndex // } // } } } }(i) } }}func (rf *Raft) Commit(N int){ rf.commitIndex = N}func (rf *Raft) SendCommitEntry(){ rf.mu.Lock() start := rf.lastApplied end := rf.commitIndex if (start &amp;gt;= end){ rf.mu.Unlock() return } rf.mu.Unlock() for i := start;i &amp;lt; end;i++{ rf.mu.Lock() if start != rf.lastApplied || end != rf.commitIndex { rf.mu.Unlock() return } msg := ApplyMsg{ CommandValid: true, CommandIndex : i + 1, SnapshotValid : false, } msg.Command = rf.Log[i - rf.LastIncludedIndex].Command rf.mu.Unlock() rf.applyCh &amp;lt;- msg } rf.mu.Lock() rf.lastApplied = end rf.mu.Unlock()}func (rf *Raft) LeaderElection(){ for { rf.mu.Lock() switch rf.state { case Follower: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout{ rf.converttoCandidate() } rf.mu.Unlock() case Candidate: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout { go rf.getVote() } rf.mu.Unlock() case Leader: rf.heartbeat() rf.mu.Unlock() } time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond) }}//// the service or tester wants to create a Raft server. the ports// of all the Raft servers (including this one) are in peers[]. this// server&#39;s port is peers[me]. all the servers&#39; peers[] arrays// have the same order. persister is a place for this server to// save its persistent state, and also initially holds the most// recent saved state, if any. applyCh is a channel on which the// tester or service expects Raft to send ApplyMsg messages.// Make() must return quickly, so it should start goroutines// for any long-running work.//func Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { rf := &amp;amp;Raft{} rf.peers = peers rf.persister = persister rf.me = me // Your initialization code here (2A, 2B, 2C). // (2A). Start. // rand.Seed(time.Now().Unix()) // rf.state = Follower rf.CurrentTerm = 0 rf.VoteFor = -1 rf.Time = time.Now() rf.Timeout = rand.Intn(200) + 400 rf.applyCh = applyCh rf.LastIncludedIndex = 0 rf.LastIncludedTerm = -1 // initialize from state persisted before a crash rf.readPersist(persister.ReadRaftState()) // if len(rf.Log) == 0{ // first_log := LogEntry{Term : -1} // rf.Log = append(rf.Log,first_log) // } // start ticker goroutine to start elections go rf.ticker() return rf}" }, { "title": "LL(1)", "url": "/posts/llvm-compiler/", "categories": "Compiler", "tags": "LLVM compiler", "date": "2021-04-21 19:10:10 +0800", "snippet": "Introduction.This project is to build a recursive descent(LL(1)) compiler with c++ language.scanner -&amp;gt; parser -&amp;gt; type checking -&amp;gt; code generation -&amp;gt; runtime system.Scanner.structure:struct token_t { int type; // one of the token codes from above std::string StringValue; int tm; int intValue; // holds lexeme value if integer float doubleValue; // holds lexeme value if double int numCount; bool isGlobal; // initialized to false; token_t(){ this-&amp;gt;numCount = 0; this-&amp;gt;isGlobal = false; }}; A files contains tons of characters. What Scanner does is to break input stream into characters and group characters into words, like in c++ file. int main(). In this start-up function, What Scanner does is to generate 4 tokens: int, main, ( and ).public API open to parser: token scanner-&amp;gt;getToken(). You can get the word token by token.Parser.In class Parser. one private member is scanner, since we should get tokens from scanner for the preparation of parsing. A parsing process is, take int main() as example again. we know that this main function has a int type and program expects a variable with same int type to be returned. I think parser does link tokens into different blocks. Here is some examples:&amp;lt;S&amp;gt; -&amp;gt; program &amp;lt;vars&amp;gt; &amp;lt;block&amp;gt;&amp;lt;block&amp;gt; -&amp;gt; start &amp;lt;vars&amp;gt; &amp;lt;stats&amp;gt; end&amp;lt;ifstat&amp;gt; -&amp;gt; if ( &amp;lt;expr&amp;gt; &amp;lt;O&amp;gt; &amp;lt;expr&amp;gt; ) &amp;lt;stat&amp;gt;For example: if statement should contain &amp;lt;expr&amp;gt; statement block.type checking.I combine type checking into parser because I don’t want the some errors happen in code generation part. Besides, although LLVM has a strong debugging tool, it is really complicated and my time is really limited. I choose to do type checking in parser part.type checking, as the word said, it is to check types. For example, a = 1. This assign statement shoud guarantee variable a is a integer. int main() should has an integer variable returned.code generation.After Parser and type checking, we can see a parser tree, it contains many blocks and many variables. What I did is to generate the code block by block with LLVM tool. After this process, a .ll file is generated and we can use clang tool to generate executable file.runtime system.Although it successfully is executed on my computer, it cannot be used in other computer. Makefile is brought up to solve this problem. Make a Makefile and make it simpler to scale to other computer../main &amp;lt;file&amp;gt; can generate a assembly file with suffix .s. clang output.s runtime.o could generate executable file.Or ./sbc.sh &amp;lt;file&amp;gt; generates an executable file directly." }, { "title": "Dark Scrum", "url": "/posts/dard_scrum/", "categories": "Large Scale Software Engineering", "tags": "Large Scale Software Engineering, paper3", "date": "2021-03-14 19:10:10 +0800", "snippet": "What is Dark Scrum.A Scrum environment should be fostered like these: a produce owner orders the work for a complex problem into a Product Backlog. The Scrum Team turns a selection of the work into an Increment of value during a Sprint. The Scrum Team and its stakeholders inspect the results and adjust for the next Sprint. Repeatthese things are ideal and are designed as a concept, it’s a different story when it goes back into the reality. That’s when Dark Scrum happens. it happens when all of the concepts of Agile and all the different aspects of teamwork are ridiculed with unreasonable expectations put forth by any number of people associated with the project.[1] Some symptoms of darkness. people know their old jobs, not their new Scrum job. Scrum generally starts with very few people trained, even fewer understanding it, and a lot of people who think they know what they’re supposed to do. It should be no surprise if they do it wrong, and very often they do.[2]. everyone knows what he is supposed to do, but there are some conflicts between programmars and power holders, like the power holders only care about what jobs need to be done, they decide what the programmars should do and put pressure on them even though programmars thinks they are not capable of doing it in a short time. In that cases, programmars try to find the shortcut to finish the job without required bug testing. the result could be: instead of the team rallying around their joint mission and sorting out a good approach for the day, someone else drags information out of them, processes it in their head, and then tells everyone what to do. Since nothing ever goes quite as we expected yesterday morning, this improper activity often comes with a lot of blame-casting and tension.[2]team doesn’t decide how much work to do. power holders have little or no idea how to program, and the programmers usually at least have some clue. They have no idea what the state of the code is, and the programmers usually do know.[2].In theory, Scrum Produce owner is supposed to meet the team frequently and decide what’s needed for the team. Programmar begins to know how to build based on the proposals. Practically, there may be even be a product owner in the team, even if there is, the product owner is not well trained.Result could be, programmars try their best to do the infinite missions, because power owner pile up the mission always. It’s no surprise that developers fail again.The team is always reminded of failure. the Dark Sprint Review begins by someone reminding everyone what the team “promised” to do. (That is, what was demanded right before the team said “We’ll try”. That’s a promise, isn’t it?) Then we look at the pitiful failure the team brings us. They make sure the programmers are made fully aware of how badly they’ve done. That will surely inspire everyone to do better next time.[2].Instead of doing things better, programmars just want to take shortcut to get all the missions finished. They implement something without bug testing, a complete design.Why Scrum is being abused always.power owners don’t see the future of the product and the only way to release the fear is to put more and more pressure on the developers.some cases that would happen.If the product has a lot of known defects, power owners assume there would be more defects. They would put pressure on the programmars.Features would come out slowly if programmar focus more on the design parts, which could make power owner feel fear.Design could fall behind if someone slow down the process.some ways to solve Dark Scrum.Acceptance Testing.Each little example becomes an acceptance test for the story or backlog item in question. After testing, programmars and product owner could know how good is the product. PO can decide the following step based on the test.Incremental Design.A small product does not need the whole design at the beginning. However, as the product grows, a big and complete design is needed. A small incremental design is required after each Sprint process. Although the Design process is really difficult, since it requires skill in design, skill in testing, and skill in refactoring.Refactoring. Refactoring is a process of transforming the code, usually in very small steps, to make it better.Refactoring is not re-writing the code, it expects a better design by the end of the current Sprint.Programmar Testing.Programmar Testing is required in addition to the Acceptance Testing. programmars could aviod mistake by Programmar Testing. The best Programmar Testing is Test-Driven Development. The step is like that: Think of some small next step on the way to our feature. Write a test that should run when that step is complete. Run it: make sure it doesn’t work now. Build the code to make the step complete. Run the test: make sure it works now. Check the code to see if it looks clear enough. If not, refactor it. Run the test again to be sure everything still works. Repeat until the feature is done.Some design problems cannot be noticed immediately, programmar testing could help us with larger refactorings.Continuous Integration. Scrum requires a running, thoroughly-tested, usable increment, containing the value of all the backlog items from previous Sprints: a complete, running, tested, operable program. Delayed integration uncovers problems, many of which require substantial debugging.how to do. People can’t just click over into Scrum’s new mode of operation. It takes time to unlearn the old ways. It takes time to learn new habits, time to learn to trust the team. It takes time for the team to learn how to be trusted: in a way that’s the underlying message of this article. Scrum’s training begins this learning process for the people who get the training.[2].Some thoughts about Dark Scrum.I read another article about someone who was forced to do Dark Scrum.[3]The summary is the project manager want to know how long it would take to complete the project, so Agile coach directed the team to estimate user story sizes on the existing backlog. The problem is team had no sense of how much time a story point was a proxy is. The result is the team had a target of a certain number of story points per sprint, which they had to meet or work overtime.Goodhart’s Law: “When a measure becomes a target, it ceases to be a good measure”. This law could describe that situation very precisely.The author said that although the leader agreed on every point he listed, and said that actually he wanted to coach the team to do the healthy thing, but that the contract which was in place prevented him from doing so. I think the real problem is everyone has a leader, your leader could have a leader, every leader don’t want to take responsible for a failure. instead of doing things right, they choose to do the things based on the old experience and don’t want to try new Scrum project. I always think that everyone has understood Scrum well in theory, they dont want to put in in practice just because the contract forced them to do the Dark Scrum. I think that’s the real problems.Reference.[1] https://www.ntaskmanager.com/blog/dark-scrum/[2] https://ronjeffries.com/articles/016-09ff/defense/[3] https://medium.com/@rogersaner/how-i-was-forced-to-do-dark-scrum-d5fdb388df4f" }, { "title": "Spanner", "url": "/posts/spanner/", "categories": "Distributed System, MIT 6824", "tags": "Distributed System, spanner", "date": "2021-03-10 16:49:10 +0800", "snippet": "Introduction.a database that shards data across Paxos state machine in data center.Features. application can specify constraints to control which datacenter contain which data. provide externally consistent reads and writes and read at the database at a time-stamp.Implementation.Architecture.universemaster: a console that displays status information about all zones for interactive debugging.location proxy: locate the spanservers assigned to serve their data.placement driver: periodically communicates with spanservers to find the data needs to be moved." }, { "title": "TiDB &amp; its community", "url": "/posts/LargeScaleEngineering_paper2/", "categories": "Large Scale Software Engineering", "tags": "Large Scale Software Engineering, paper2", "date": "2021-02-24 19:10:10 +0800", "snippet": "Summary.TiDB is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability. This project has more than 26.9k stars. repository linkREADME.md filethere are some tabs in this file for starters to use or for the contributor to contribute to this project.What is TiDB: introduce what does TiDB do and some core features of TiDB.Quick Start: Made for users and tells the users how to set up environments.To Start developing TiDB: Made for contributors. it shows how does the contributors contribute to TiDB.[1]case studies: Made for learners, someone has to understand TiDB before he becomes a contributor.blog: posts something interesting on the blog for everyone interested in this project.documentation: provide all the documents on these tabs.TiDB monthly: post something like a blog about monthly news of TiDB.Architecture: for starters. help starters understand the whole structure of TiDB.For starters, they can quickly know what the TiDB is and how to deploy environments through quick start tab, instead of looking through the whole code or the whole documents.For contributors, they can know how to make contributions to the project through Quick Start tab instead of emailing the founder all the time.For leaders, they can post the most recent news about TiDB to get everyone to know what the future is and what TiDB team is doing.It can make people in different roles collaborate well with each other in this layout.how do contributors communicate with each other? Slack channel.TiDB has created a Slack community for any members to communicate with each other. this community has grown up to 2500 members and every member can share anything on Slack. This community is more like a chatting box, when someone meets some personal technical problems like the environment is set up unsuccessfully, he could go to this community for help and the problems are always being solved in minutes. Issues tab.every project has a tab named with issues. everyone could go to this tab posting an issue on it and wait for other contributors to deal with it. At the same time, every contributor can help others fix the problems. The issues are always formal and usually, no one meets the same problems before. The issues are always bugs waiting for someone to solve them out. Community. This repository has been created for contributors to contribute to the project. the details are delivered later. there are some other ways, such as Reddit, Twitter, and email. some news or some activities are posted on this media website and make everyone knows what TiDB teams have been doing recently.how to make a contribution. there is a document named with how to contribute. Contributors have to read this guideline before contributing. This document help contributors make contributions efficiently. Contributors could ask for any helps through the ways shown above when they meet any kinds of problems. Submitting a pull request after all the tests are running successfully. Waiting for the leader to accept the request and merge it to a repository.Learning Resources.For starters, they could everything about TiDB they want which can help them learn and contribute to TiDB. there are two versions: Chinese and English versions.Community.[2]Community is really important for this project. It is efficient for contributors to collaborate.Architecture.the architecture shows the structure of the whole community. Every member are responsible for different parts.PMC.PMC is short for Project Management Committee. PMC is the core management team and oversees TiDB community. it is responsible for making the whole process work smoothly and doing the decision-making about what the future of TiDB goes to.Organizer CommitteeOrganizer Committee is in charge of community events or activity operations, they ensure the execution of PMC’s strategies and decisions.TiDB user group.3 roles. Leader, Co-leader, and Ambassador. The TiDB User Groups (TUGs) are groups for facilitating communication and discovery of information related to topics that have long-term relevance to large groups of TiDB users. Leader makes overall plans and tracks progress. Co-leader shares TUG responsibilities. Ambassador are passionate about sharing and promoting on blogs.TiDB developer group.5 roles: Maintainer, Committer, Reviewer, Active Contributor and Contributor. Contributor, and Active Contributor are responsible for making any PRs to the community. Reviewer are responsible for reviewing the code to ensure quality and correctness. Maintainers are the planners and designers of the TiDB project, with the authority to merge branches into the master.BDFL.BDFL is short for Benevolent dictator for life. Actually, the members in PMC are BDFL. they are responsible for decision-making and retaining the final say in disputes or arguments within the community.Governance.Governance is followed by all the repositories in TiDB.two groups: Special Interest Groups (SIGs) are persistent open groups that focus on a module of TiDB.Working Groups (WGs) are temporary groups that are formed to address issues that cross SIG boundaries.Principle.Open: TiDB is open-source.Welcoming and respectful: welcome everyone to make a contribution and respect contributors.Transparent and accessible: Work and collaboration are done in public.Merit: Good contribution can be accepted.Code of conduct. In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identit, and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.[3].Decision-making and voting.Proposals and ideas can be submitted for agreement via a github issue or PR. Major changes such as feature proposals and organization or process changes should be brought to the PMC or Maintainers’ attention through the Request for Comments (RFC) process. For the change to happen, the RFC must earn the supermajority (2/3) votes in the corresponding group.[3].Conflict resolution.the conflict happens when the agreement is not achieved by everyone. Voting comes in when dealing with this situation based on Principle and Code of Conduct.Adding new project.the projects can be added via issue discussion. Once sufficient discussions have taken place, the maintainer should decide whether the new project should be added or not.Roadmap.Roadmap is like a route where the TiDB goes on. Roadmap is really an important part in TiDB. It is made by PMC and tells tons of contributors what to do in the future. When one task is done, a tick should be listed before the task. In that case, every member could see the progress TiDB makes.Comunity Activities.it lists all the activities since the communities have been set up. it tells all the members the activities are being held in the future. In this way, it can make members being more passionate about contributions to the projects.RFC.RFC is short for request for command. TiDB community requests everyone to upload a markdown file to the remote repository when they make a contribution to TiDB project. It would let other contributors know what does the code do easily.Conclusion.as of today, TiDB has grown up to be a large cloud database. The progress is achieved by the combination of leaders, contributors and Users. Community plays an important role in the development of TiDB.Reference.[1] https://github.com/pingcap/community/blob/master/communicating.md[2] https://github.com/pingcap/community[3] https://github.com/pingcap/community/blob/master/GOVERNANCE.md" }, { "title": "Waterfall vs Agile", "url": "/posts/LargeScaleEngineering_Paper1/", "categories": "Large Scale Software Engineering", "tags": "Large Scale Software Engineering, paper1", "date": "2021-01-28 19:10:10 +0800", "snippet": "Summary.Waterfall and Aigle are two methologies which are largely used in industry, market and courseworks. Which methology is goint to use is always a question which is needed to be further discuessed. This paper is not going to talk about which one is right and the other one is wrong. This paper is going to focus on the difference between Waterfall and Agile and focus on the developing process of the whole project. Both of them can help a development team produce a high-quality software.What is Waterfall methodology.Waterfall methodology is a sequential approach that divides the SDLC to distinct phases such as requirements gathering, analysis and design, coding and unit testing, system and user acceptance testing, and deployment. The next phase can only proceed if the previous phase has been completed. In between phases, a deliverable is expected or a document is signed off. All phases are passed through and completed only once, so all requirements are gathered as much as possible at the start to provide the information in creating the plans, schedules, budget, and resources. It is plan-driven, so any changes after the project has started would offset the original plan and require a restart.What is Agile methodology.Agile is a practice that helps continuous iteration of development and testing in the software development process. Development and testing activities are concurrent, unlike the Waterfall model which means more communications are allowed between customers.Difference. [1]Difference reference Agile is a methodology which follows an incremental approach while Waterfall is sequential design process. Agile is known for flexibility while Waterfall is a solid structured software development methodology. Agile performs testing concurrently with software development whereas in Waterfall methodology testing comes after the “Build” phase. Agile allows changes in project development requirement whereas Waterfall has no scope of changing the requirements once the project development starts.Advantage. [2]Waterfall vs AgileFor Waterfall: it is straight planing and designing due to the agreement on deliverables at the start of the project. Better design with whole-system approach. Defined scope of work. Easier costing. Clear measurements of progress. For Agile: Faster software development life cycle Predictable schedule in sprints Customer-focused, resulting in increased customer satisfaction Flexible in accepting changes Empowers teams to manage projects Promotes efficient communications Disadvantage. [2]Waterfall: Rigid structure to allow necessary changes No allowance for uncertainty Limited customer engagement, resulting in poor satisfaction Sequential approach is not ideal for a large-sized project where the end result is too far in the future Testing is done only at the latter phases of the project. Agile: Agile requires a high degree of customer involvement, which not all customers are comfortable with or prefer to give. Agile assumes every team member is completely dedicated to the project, without which weakens the principle of self-management. A time-boxed iteration may not be enough to accommodate all deliverables, which will require changes in priority and additional sprints that can bring up cost. Agile recommends co-location for efficient communication, which is not always possible. Examples.Waterfall:I never worked in a company, so I don’t know how does Agile or Waterfall look like in company,but I think Agile or Waterfall support for company is really complex and beyond my imagination. For the following parapragh I just want to take some courseworks as examples.In a way, I don’t even notice some works I have done is actually using Waterfall methodology.For the course about distributed system: Link,the whole structure is using waterfall methodology, it makes a schedule that what students should do in each day. (Advantage 2). Actually, it was divided into several phases based on weeks. Advantage 1. When I firstly see this page, I can realise what I am supposed to learn during this course and there is a clear target in my mind that I know what I can do after taking this course. I think this course is really good for the students who have no knowledge about distributed system.For the figure above, it defined the scope of the lecture, such as Video and Preparation. I know what I should do before watching the video, I need to read the paper about GFS first. Actually, it use waterfall methodology, since if you don’t read the paper before you go to the video link, you can’t understand the lecture well. (Advanteage 3) Moreover, it is really easy for everyone to measure their progress, since you know whether you have read the paper or not and whether you understand well about lecture or not. (Advantage 5).For the lab part, there are 4 parts: lab1 to lab4.The whole lab is to build a key/value storage database and it is divided into 4 labs. the most important thing is that if you do not finish lab1, let’s say there is a bug in lab1. There has to be a problem when you go into lab2 if you do not deal with that problem in lab1, which matches the waterfall methodology. Each phase should be reviewed and confirmed before you go into the next phase.(Advantage 5)Each coins have two sides, although everyone can get a lot after taking all the lectures and finishing all the labs, it is really hard to achieve it. Firstly, Golang is really new language and maybe no students have a experience about it. This course never takes this into consideration. For me, I feel frustrated when I spend 1 week on the golang. (Disadvantage 2), it has no allowance for uncertainty.Worsely, only if you write a code without bugs, you can pass all the tests. It means that there is no way for you to find where is the problem. (Disadvantage 5).I think it would be better if this course combines Waterfall and Agile into the course design part.Agile:I can’t think of a specific example which is only use Agile. I just recall a simple part which include Agile. In this course, professor opens a discussion tabs and asks if the amount of reading is ok for everyone, I think it a part of Agile.A popular common example is Scrum.Scrum is a hands-on system consisting of simple interlocking steps and components: Example: Bill meets with a customer to discuss her company’s needs. Those needs are the product backlog. Bill chooses the most important tasks to work on in the next two weeks. His team meets in a daily scrum to target work for the day ahead and address roadblocks. At the end of the sprint, Bill delivers the work, reviews the backlog, and sets the goal for the next sprint. The cycle repeats until the software is complete. [4]Mixing Agile and Waterfall.It’s rare to see a software that follows a single methodology. Both of them are right or resonable for their organizaions. Agile and Waterfall can not only coexist but also play well together.Conclusion.Although Agile and Waterfall are two different methodologies which apply to the software projects design and developments. The purposes of them is to ensure the end product of high quality.Waterfall methodology is mainly used in a situation: the team has specific and straight-forward target, like I need to implement something based on paper, because no customer-involvement is needed in this process.Agile methodology is mainly used in a situation: the work has some kind of uncertainty, like the team needs some feedback from customers or leaders.Reference.[1] https://project-management.com/agile-vs-waterfall/[2] https://www.guru99.com/waterfall-vs-agile.html[3] https://techbeacon.com/app-dev-testing/managing-agile-waterfall-together[4] https://stackify.com/agile-methodology/" }, { "title": "Amazon Aurora Note", "url": "/posts/Aurora/", "categories": "Distributed System, MIT 6824", "tags": "Distributed System, Aurora", "date": "2021-01-19 09:46:10 +0800", "snippet": "Abstract.Aurora is a service as part of AWS. The bottleneck of the performance is moving from storage, kind of things to network. It was pushing redo processing to a multi-tenant scale-out storage service, purpose-built for Aurora.Introduction.since I/Os can be spread across many nodes, each node are longer hot. Network is become the bottleneck of the performance of the whole system. Aurora leverage the redo log across a highly-distributed cloud environment.Durability at scaleReplication and Correlated Failures.quorum-based voting protocol: read + write &amp;gt; nodes and write &amp;gt; nodes / 2;In Aurora, suppose there is 6 nodes and 3 AZ. each AZ has 2 nodes. and each node is the copy of the other one. According to quorum-based voting protocol. one AZ failure is allowed and 3 nodes can failure.Segmented Storage.MTTF: mean time to failures, it is hard to reduce the possibility of MTTF.MTTR: mean time to repair, instead, Aurora focuses on reducting the possibility of MTTR.database is segmented into PGs, and each PG has six 10GB segments, 3 AZs. A large node is required. the Node is provisioned as virtual hosts using EC2.Operational Advantages of Resilience.if a system can handle long failures, it definitely handle short failure. No more than 1 PGs can be patched simultaneously.The log is the database.the burden of amplified writes.MySQL: generates many I/Os, result in additive latencies and synchronous stalls.MySQL writes data pages to objects as well as redo log record consisting of two parts. before and after images.Offloading Redo Processing to Storage.In Aurora, the only writes across the network is redo log instead of the whole object.crash recovery: the process spread across all normal foreground processing.Storage Service Design PointsIn Aurora, the background processing has negative correlation with foreground processing.Traditionally, the background processing has positive correlation with foreground processing.the log marches forward.show how to keep consistency without expensive redo processing in Aurora.Solution sketch: Asynchronous Processing.each log record has an associated Log Sequence Number(LSN) that is a monotonically increasing value.In that case, single segmented read is allowed instead of quorum read.Normal Operation.VCL: Volume Complete LSN.Writes.A special log LSN is to contrains the database get too far ahead of the whole system. it can’t not be greater than the sum of current VDL. (normally 10 million).Commitsthe commits are processing asynchronously.Reads.a page in the buffer cache must always be of the latest version.Replicas.Recovery.Traditionally, the database depends on WAL(write-ahead log) to represent the precise content of all committed transactions.checkpoint log applicator will apply checkpoint log to the relevant database periodically. When in the way fo recovery, the database could roll back to that checkpoint.Putting in all together.need to be detailed further." }, { "title": "6.824 lab3A", "url": "/posts/6824_lab3A/", "categories": "Distributed System, MIT 6824", "tags": "Distributed System, kvservice", "date": "2021-01-15 09:46:10 +0800", "snippet": "SummaryLab3A require us to implement 3 api: put,append and get. we should maintain linearizability.linearizability: if append: a-&amp;gt;13 is executed before get: a, get should return 13.To avoid the same function being executed twice, we should allocate unique ID for each request.Process.careful: kvservers cannot communicate with each other directly.Result.Code.server.go:package kvraftimport ( &quot;../labgob&quot; &quot;../labrpc&quot; &quot;log&quot; &quot;../raft&quot; &quot;sync&quot; &quot;sync/atomic&quot; &quot;time&quot;)const Debug = 0func DPrintf(format string, a ...interface{}) (n int, err error) { if Debug &amp;gt; 0 { log.Printf(format, a...) } return}type Op struct { // Your definitions here. // Field names must start with capital letters, // otherwise RPC will break. Operation string // put Append or Get Key string Value string ClientId int64 SeqId int64}type OpInfo struct { ClientId int64 SeqId int64 Value string}type KVServer struct { mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg dead int32 // set by Kill() maxraftstate int // snapshot if log grows this big // Your definitions here. kvDB map[string]string kvClient map[int64]int64 // map clientId to SeqId kvResponse map[int]chan OpInfo // map SeqId to Op to avoid second response.}func (kv *KVServer) getchannel(index int) chan OpInfo{ kv.mu.Lock() defer kv.mu.Unlock() ch,ok := kv.kvResponse[index] if !ok{ ch = make(chan OpInfo,1) kv.kvResponse[index] = ch } return ch}func (kv *KVServer) Get(args *GetArgs, reply *GetReply) { // Your code here. // define a Op structure. GetOp := Op{ Operation : &quot;Get&quot;, Key : args.Key, ClientId : args.ClientId, SeqId : args.SeqId, } // connect to raft. index,_,isleader := kv.rf.Start(GetOp) if !isleader { reply.Err = ErrWrongLeader return } ch := kv.getchannel(index) defer func(){ kv.mu.Lock() delete(kv.kvResponse,index) kv.mu.Unlock() }() for { select { case msg := &amp;lt;- ch: if msg.ClientId == args.ClientId &amp;amp;&amp;amp; msg.SeqId == args.SeqId{ reply.Err = OK reply.Value = msg.Value return } else { reply.Err = ErrWrongLeader return } case &amp;lt;-time.After(1000 * time.Millisecond): reply.Err = ErrWrongLeader return } }}func (kv *KVServer) PutAppend(args *PutAppendArgs, reply *PutAppendReply) { // Your code here. // define a Op structure. PutAppendOp := Op{ Operation : args.Op, Key : args.Key, Value : args.Value, ClientId : args.ClientId, SeqId : args.SeqId, } // connect to raft. index,_,isleader := kv.rf.Start(PutAppendOp) if !isleader { reply.Err = ErrWrongLeader return } ch := kv.getchannel(index) defer func(){ kv.mu.Lock() delete(kv.kvResponse,index) kv.mu.Unlock() }() for { select { case msg := &amp;lt;- ch: if msg.ClientId == args.ClientId &amp;amp;&amp;amp; msg.SeqId == args.SeqId{ reply.Err = OK return } else { reply.Err = ErrWrongLeader return } case &amp;lt;-time.After(1000 * time.Millisecond): reply.Err = ErrWrongLeader return } }}//// the tester calls Kill() when a KVServer instance won&#39;t// be needed again. for your convenience, we supply// code to set rf.dead (without needing a lock),// and a killed() method to test rf.dead in// long-running loops. you can also add your own// code to Kill(). you&#39;re not required to do anything// about this, but it may be convenient (for example)// to suppress debug output from a Kill()ed instance.//func (kv *KVServer) Kill() { atomic.StoreInt32(&amp;amp;kv.dead, 1) kv.rf.Kill() // Your code here, if desired.}func (kv *KVServer) killed() bool { z := atomic.LoadInt32(&amp;amp;kv.dead) return z == 1}//// servers[] contains the ports of the set of// servers that will cooperate via Raft to// form the fault-tolerant key/value service.// me is the index of the current server in servers[].// the k/v server should store snapshots through the underlying Raft// implementation, which should call persister.SaveStateAndSnapshot() to// atomically save the Raft state along with the snapshot.// the k/v server should snapshot when Raft&#39;s saved state exceeds maxraftstate bytes,// in order to allow Raft to garbage-collect its log. if maxraftstate is -1,// you don&#39;t need to snapshot.// StartKVServer() must return quickly, so it should start goroutines// for any long-running work.//func StartKVServer(servers []*labrpc.ClientEnd, me int, persister *raft.Persister, maxraftstate int) *KVServer { // call labgob.Register on structures you want // Go&#39;s RPC library to marshall/unmarshall. labgob.Register(Op{}) kv := new(KVServer) kv.me = me kv.maxraftstate = maxraftstate // You may need initialization code here. kv.applyCh = make(chan raft.ApplyMsg) kv.rf = raft.Make(servers, me, persister, kv.applyCh) kv.kvDB = make(map[string]string) kv.kvClient = make(map[int64]int64) kv.kvResponse = make(map[int] chan OpInfo) // You may need initialization code here. go kv.check() return kv}func (kv *KVServer) check(){ for{ select { case msg := &amp;lt;- kv.applyCh: kv.mu.Lock() DPrintf(&quot;receive from raft&quot;) Info := OpInfo{} op := msg.Command.(Op) Info.ClientId = op.ClientId Info.SeqId = op.SeqId if op.SeqId &amp;gt; kv.kvClient[op.ClientId]{ kv.kvClient[op.ClientId] = op.SeqId if op.Operation == &quot;Put&quot;{ kv.kvDB[op.Key] = op.Value } if op.Operation == &quot;Append&quot;{ if val,existed := kv.kvDB[op.Key];existed{ kv.kvDB[op.Key] = val + op.Value } else { kv.kvDB[op.Key] = op.Value } } } if op.Operation == &quot;Get&quot; { Info.Value = kv.kvDB[op.Key] } kv.mu.Unlock() DPrintf(&quot;send something to channel&quot;) kv.getchannel(msg.CommandIndex) &amp;lt;- Info DPrintf(&quot;end channel&quot;) default: time.Sleep(time.Duration(1) * time.Millisecond) } }}common.go:package kvraftconst ( OK = &quot;OK&quot; ErrNoKey = &quot;ErrNoKey&quot; ErrWrongLeader = &quot;ErrWrongLeader&quot;)type Err string// Put or Appendtype PutAppendArgs struct { Key string Value string Op string // &quot;Put&quot; or &quot;Append&quot; // You&#39;ll have to add definitions here. // Field names must start with capital letters, // otherwise RPC will break. ClientId int64 SeqId int64}type PutAppendReply struct { Err Err}type GetArgs struct { Key string // You&#39;ll have to add definitions here. ClientId int64 SeqId int64}type GetReply struct { Err Err Value string}client.go:package kvraftimport &quot;../labrpc&quot;import &quot;crypto/rand&quot;import &quot;math/big&quot;import &quot;sync&quot;import &quot;sync/atomic&quot;import &quot;time&quot;type Clerk struct { mu sync.Mutex servers []*labrpc.ClientEnd // You will have to modify this struct. Leader int // initialize to -1 if Clerk does not know which one is leader. ClientId int64 SeqId int64}func nrand() int64 { max := big.NewInt(int64(1) &amp;lt;&amp;lt; 62) bigx, _ := rand.Int(rand.Reader, max) x := bigx.Int64() return x}func MakeClerk(servers []*labrpc.ClientEnd) *Clerk { ck := new(Clerk) ck.servers = servers // You&#39;ll have to add code here. ck.Leader = -1 ck.ClientId = nrand() return ck}//// fetch the current value for a key.// returns &quot;&quot; if the key does not exist.// keeps trying forever in the face of all other errors.//// you can send an RPC with code like this:// ok := ck.servers[i].Call(&quot;KVServer.Get&quot;, &amp;amp;args, &amp;amp;reply)//// the types of args and reply (including whether they are pointers)// must match the declared types of the RPC handler function&#39;s// arguments. and reply must be passed as a pointer.//func (ck *Clerk) Get(key string) string { // You will have to modify this function. ck.mu.Lock() defer ck.mu.Unlock() args := GetArgs{ Key : key, ClientId : ck.ClientId, SeqId: atomic.AddInt64(&amp;amp;ck.SeqId, 1), } // send to kvserver. var index int if ck.Leader != -1{ index = ck.Leader } else { index = 0 } // at this time. I do not care about timeout. for { reply := GetReply{} ok := ck.servers[index].Call(&quot;KVServer.Get&quot;, &amp;amp;args,&amp;amp;reply) if !ok{ index = (index + 1) % (len(ck.servers)) continue } if reply.Err == OK { ck.Leader = index return reply.Value } if reply.Err == ErrNoKey { return &quot;&quot; } if reply.Err == ErrWrongLeader { index = (index + 1) % (len(ck.servers)) } time.Sleep(100 * time.Millisecond) }}//// shared by Put and Append.//// you can send an RPC with code like this:// ok := ck.servers[i].Call(&quot;KVServer.PutAppend&quot;, &amp;amp;args, &amp;amp;reply)//// the types of args and reply (including whether they are pointers)// must match the declared types of the RPC handler function&#39;s// arguments. and reply must be passed as a pointer.//func (ck *Clerk) PutAppend(key string, value string, op string) { ck.mu.Lock() defer ck.mu.Unlock() // You will have to modify this function. args := PutAppendArgs{ Key : key, Value : value, Op : op, ClientId : ck.ClientId, SeqId: atomic.AddInt64(&amp;amp;ck.SeqId, 1), } var index int if ck.Leader != -1{ index = ck.Leader } else { index = 0 } for { reply := PutAppendReply{} ok := ck.servers[index].Call(&quot;KVServer.PutAppend&quot;, &amp;amp;args, &amp;amp;reply) if !ok { index = (index + 1) % (len(ck.servers)) continue } if reply.Err == OK || reply.Err == ErrNoKey{ ck.Leader = index return } if reply.Err == ErrWrongLeader { index = (index + 1) % (len(ck.servers)) } time.Sleep(100 * time.Millisecond) }}func (ck *Clerk) Put(key string, value string) { ck.PutAppend(key, value, &quot;Put&quot;)}func (ck *Clerk) Append(key string, value string) { ck.PutAppend(key, value, &quot;Append&quot;)}" }, { "title": "CRAQ Note", "url": "/posts/CRAQ/", "categories": "Distributed System", "tags": "Distributed System, CRAQ", "date": "2021-01-06 21:32:10 +0800", "snippet": "Abstract.Many systems sacrifice consistency property for greater availability and higher throughput. CRAQ can maintain strong consistency while improving throughput.Introduction.Object-based system : supported by key-value system. Easy to modify, cheap to provide consistency guarantees.chain replication : chain head handles write operations and tail handles read operatons. (guarantee consistency).Problems: if all reads go to one node, the node would become hotspots.CRAQ(that part I still don’t know well).dividing read operations all over nodes in a chain and keep eventual-consistency at the same time.Basic System Model.Interface and Consistency Modeltwo APIs : write and read.write and read are in some specific sequential order to guarantee strong consistency.Chain Replication.replicating data across mutiple nodes and keep strong consistency at the same time.Read can only get the committed value where write propogate to the tail of the chain. Advantage: strong consistency. Disadvantage: reduce read throughput to that of a single node, instead of scale out with chain size.Chain Replication with apportioned Queries.Read operations can be at any nodes. When writes propogate along the chain, the version is marked dirty until tail commit it. Version could be marked clean when the acknowledge go back up the chain. When a read operation comes at a node where the version is dirty, the node could ask tail node for the latest version. all in all, the latest committed value is returned always.Could be applied in two scenarios: Read-mostly Workloads. Write-heavy Workloads. Guarantee version queries is light-weight than full reads.Consistency Models on CRAQ. strong consistency.(listed above) Eventual consistency. Eventual consistency with maximum-bounded inconsistency.Failure Recovery in CRAQ.prodecessor takes over the tail, successor takes over the head.Scaling CRAQChain Placement Strategies.note: Popular objects might need to be heavily replicated while unpopular ones can be scarce.two identifiers: chain identifier and key identifierchain identifer determines which nodes in CRAQ will store keys within the chain.key identifier determines provides unique naming within the chain.CRAQ within a Datacenter.place chains within a datacenter and map all the chain identifier to single head node.CRAQ across multiple datacenters.Zookeeper Coordination Service.Zookeeper: CRAQ nodes should receive a notification when nodes are added or deleted in the group.Problem: Placing multiple ZooKeeper nodes within a single datacenter improves Zookeeper read scalability within that datacenter, but at the cost of wide-area performanceeach datacenter can contain his own local Zookeeper instance of multiple nodes.Extensions.mini-transaction on CRAQ.Key extension supports transactional operations.Single-key Operation.three operations: Prepend/Append, Increment/Decrement and Test-and-Set.for Prepend/append and Increment/Decrement, the head can apply the operations to latest version even if the version is dirty.for Test-and-Set, the request is rejected if the version is not matched.if the requests are frequent, the head can buffer the requests and batch the updates. the overhead is expensive.Single-chain Operation.A optimistic two-phase commit protocol. A linear address is being implemented in several nodes (fault-tolerance). if the address is locked, the protocol commit.Multi-Chain Operations.the chain head can lock any keys involved in the minitransaction.Lowering Write latency with Multicast.Multicast can improve write performance.(at this time, I don’t care about the Implementation and Evaluation of CRAQ)Management and Implementation." }, { "title": "ZooKeeper Note", "url": "/posts/Zookeeper/", "categories": "Distributed System", "tags": "Distributed System, Zookeeper", "date": "2021-01-02 23:17:10 +0800", "snippet": "AbstractZookeeper: high performance instead of wait-free property. FIFO requests and linearizability for all requests.Introduction.Instead of implementing the server side, Zookeeper opted to implement application side. That implementation of coordination kernel enables primitive without requesting change to core service. In Zookeeper, blockint primitive are moved away because wait-free property is really important for fault-torelance and high performance. Method: FIFO client ordering, linearization.Zookeeper Service.Service Overviewsequential flag: monotonically increasing counter. Watches: one-time triggers associated with a session, used to tell the client that the value is changed even though the value is returned.Data Model: Not designed for general data storage. Instead, it map to the abstraction of the tree, (metadata)metadata: configuration info, shared data storage location, version of znode.Session: Initialized when client connects to Zookeeper. Timeout is provided and state changes are observed within the session.znode is organized in hierarchical name space tree.client can create two znode: Regular and Ephemeral.Client APIcreate(path,data,flags): create a znode.delete(path,version) : delete a znode at expected version.exists(path, watch) : return true if znode is existed.getData(path, watch) : Return data.setData(path, data, version) : write something to znode.getChildren(path, watch) : return name of the set and children of a node.sync(path) : waits for all updates pending at the start to propagate to the server.Zookeeper has synchronous and asynchronous version. Failure happens if the version is not matched.all API has a parameter named path, which means that zookeeper access znode through path. In that case, some API like open and close could be simplified and some extra data can be eliminated.Zookeeper guarantees.two guarantees: Linearizable writes and FIFO client order..two main points : 1. other processes cannot use configuration if one process make a change to it. 2. a process can only use whole process instead of partial process.one problem can be further discussed: what if a process read a ready node and start reading and at the same time a leader make a change to it. (ordering guarantee for notifications.)Examples of primitives.the paper lists some of primitives, such as Configuration Management, Rendezvous, Group Membership, Locks. all these primitives show one key thought, a process can create a new ephemeral znode with some metadata.simple locks: many clients would compete for the locks with herd effect.Locks without herd effect : all the requests are lined up. Double Barrier : synchronize the beginning and the end of computation.Zookeeper Applications.Fetching ServiceZookeeper is used to manage the configuration metadata. When decoupling clients from servers, it guarantee the requests go to healthy service.KattaDivide the work of indexing using shards.Yahoo! Message Broker.manage three parts: configuration metadata, failure detection and group membership.Zookeeper implementation.read requests: serviced from local replica of server database.write requests: serviced from agreement protocol. (leader broadcasts to followers and replicate.)Request Processorfuture state, version must be calculated when applying something to state machine to guarantee consensus. If successful TXN should be returned and otherwise, errorTXN is returned.Atomic Broadcast.changes to Zookeeper are throught Zab. To keep high throughput, Zookeeer keep processing pipeline full and guarantee the changes are in the order the clients sent.Normally, Zab delivered all the messages once, but it does not know whether the message is delivered successfully or not.Replicated DatabaseSnapshot is required in case of recovery.Client-Server Interactionsnote: when one write is processing, no other write and read are executing concurrently. when one read operation is returned, a tagged parameter named zxid corresponding to last transaction is returned. a stale value maybe returned if using fast reads. Solved by sync. timeout.(at this time, I care less about the performance of the Zookeeper.)" }, { "title": "Computernetworknote", "url": "/posts/computerNetworkNote/", "categories": "", "tags": "", "date": "2021-01-01 00:00:00 +0800", "snippet": "Chapter 1.packet-switching: hosts break application-layer messages into packets.circuit switching: FDM and TDM.packet delay: 4 types of delay: nodal processing, queueing delay, transmission delay, propagation delay.5 layers: application –&amp;gt; (presentation) –&amp;gt; session –&amp;gt; transport –&amp;gt; network –&amp;gt; link –&amp;gt; physical.application : FTP, SMTP, HTTP.presentation : encryption.session : synchronization.transport : TCP, UDP.network : IP and routing protocol.link : Ethernet.physical : ….Application.Q: what transport service does an app need?data integrity, timing(interative games), throughput(multimedia), security.TCP service.flow control, congestion control, do not provided: timing, minimum throughput, security.UDP service.no flow control and congestion control.HTTP: TCP connection. RTT(time for a small packet to travel from client to server and back.) Time: 2RTT + transmission time. Persistent HTTP: leaves connection open after sending response. server maintains no information about past client requests.HTTP/1.0 and HTTP/1.1: Both: Get, post, head. HTTP/1.1: put, delete.Cookie:4 components: cookie header line of HTTP response message. cookie header line of next HTTP response message. cookie kept on host. back-end database at website.DNS: map ip to name;Transport Layersummary: two transport layer protocol, TCP and UDP.Difference between network layer and transport layer. logical communication between app processes. (transport layer)communication between hosts. (network layer.)provide logical communication between app processes.break app messages into segments and passes it to network layer.two things cannot be guaranteed: delay and bandwidth.Multiplexing/Demultiplexing.multiplexing: handle data from multiple sockets.demultiplexing: send the segment to correct socket from the header info.TCP: congestion control, flow control, and connection setup.Multiplexing/Demultiplexing.multiplexing: handle data from multiple sockets, add transport header.demultiplexing: use header info to deliver received messages to correct socket.UDP.no handshaking between UDP sender. 8 bytes.error-detected: checksum..Q: Why does UDP provide a checksum as many link layer protocols (e.g., the popular Ethernet protocol) also provide error checking?A: No guarantee that all the links between source and destination provide error checking.Pipelined protocol.two protocol. (one is to send the whole N packet and the other is to send the exact one.) Go back N: receiver only sends culmulative ack. sender has timer for oldest unacked packet. selective repeat: receiver sends individual ack for each packet.TCP.Network layer.two plane: data plane and control plane.scheduling mechanismFirst in first out.network layer.routing protocol, IP protocol, ICMP protocol.key abstract: data format, fragmentation, IPv4 addressing, network addressing translation, Ipv6.Ip protocol.20 bytes for ip 4. 40 bytes for ip6.Q: DHCP process: broadcast-&amp;gt; get a respond -&amp;gt; request IP address -&amp;gt; server sends back ack msg.control panel.per-router control plane and logically centralized control plane.routing protocols.determine ‘good’ paths from sending host to receiving host.Dijkstra’s algorithm.link costs known to all nodes.AS: autonomous systems.inter-AS: BGP. intra-AS: OSPF.eBGP: neighboring iBGP: all AS-internal routers.BGP. (TCP)AS-Path, next-hop.Q: Why a logically centralized control plane?easier network managementtable based forwarding.distributed is more difficult.SDN.data plane switch -&amp;gt; controller -&amp;gt; control application.fast, simple, commodity,OpenFLow protocol. operates between controller, switch. based on TCP.ICMP. internet control message protocol.SNMP protocol.Link Layer.features: error detection, correction. sharing a broadcast channel. Ethernet. summary: communication channel that connect adjacent nodes. multiple links, Ethernet on the first link, and 802.11 on the last link.service.encapsulate datagrams into frame, adding header, MAC address.reliable delivery.flow control.. error detection and error correction.Error detection.combine EDC to datagram.sender puts checksum value into UDP field.cyclic redundancy check.multiple access protocol.two types: point-to-point, broadcast.single shared broadcast channel.collision if node receives two or more signals at the same time.MAC protocol. channel partitioning. TDMA(time division multiple access.) and FDMA. random access. slotted ALOHA. efficienty: 0.37. nodes start to transmit only slot beginning. pros: simple. Cons: collisions, idle slots. need closk synchronization. CSMA(carrier sense multiple access): listen before transmit. still with collision. CSMA/CD. better for wired LANs. better than ALOHA. taking turns. polling: a master. token passing. token passed from one node to next node. cable access network.LANs.MAC addresses and ARP.48 bits. ARP: address resolution protocol.IP node(host router) &amp;lt;IP, MAC, TTL&amp;gt;Ethernet.preamble: 7 bytes.source address: 6 bytes .connectionless, no handshaking between sender NICs and receiver NICs. unreliable: no Acks.table: &amp;lt;MAC, interface, TTL&amp;gt;VLANs: virtual local area network.port-based VLAN.traffic isolation, dynamic membership, forwarding between VLANs.link virtualizationMPLS. multiprotocol label switching.MPLS can be based on source and destination address." }, { "title": "Raft note", "url": "/posts/raft/", "categories": "Distributed System", "tags": "Distributed System, Raft", "date": "2020-12-31 17:41:10 +0800", "snippet": "Why Raft?Paxos can provide correctness and safety, it is capable of reaching agreement on a single decision. However, it is difficult to understand and it does not provide a good foundation for implementations. (a bad foundation can cause developer waste time on debugging.)Raft consensus algorithm. Raft basics.Follower only receives request from Leader.Candidate if no leader in the term, follower could become CandidateLeader Control the follower. Log replication. (5.3 Log entries)summary: A log is committed: entry has been replicated on a majority of servers. leader could respond to client before follower store its log entries. if two log entries have same index and term, these two entries have the same command and identical previous log entries. consistency check:Normally, logs of Leader and Follower should stay consistent.To preserve Log Matching Property:(if leader crashes for some predicted reasons) forcing followers to duplicate the leader’s logs. find the latest log in which followers and leader are same, delete log entries after it and duplicate leader’s log entries to it. Safety. (5.4 Safety)problem: the previous section cannot guarantee all servers execute same command in the same order. Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries. Note: Only log entries from the leader’s current term are committed by counting replicas Log compaction. (7. Log compaction)Problems: in practical world, log cannot grow without bound, if log grows longer, it occupied more space and toke more time to replay.Code.I would optimize it if I have free time. (the performance is not really good)package raft//// this is an outline of the API that raft must expose to// the service (or tester). see comments below for// each of these functions for more details.//// rf = Make(...)// create a new Raft server.// rf.Start(command interface{}) (index, term, isleader)// start agreement on a new log entry// rf.GetState() (term, isLeader)// ask a Raft for its current term, and whether it thinks it is leader// ApplyMsg// each time a new entry is committed to the log, each Raft peer// should send an ApplyMsg to the service (or tester)// in the same server.//import &quot;sync&quot;import &quot;sync/atomic&quot;import &quot;../labrpc&quot;// new import.import &quot;time&quot;import &quot;math/rand&quot;import &quot;bytes&quot;import &quot;../labgob&quot;// 3 states for servers.const Leader = 0const Candidate = 1const Follower = 2//// as each Raft peer becomes aware that Successive log entries are// committed, the peer should send an ApplyMsg to the service (or// tester) on the same server, via the applyCh passed to Make(). set// CommandValid to true to indicate that the ApplyMsg contains a newly// committed log entry.//// in Lab 3 you&#39;ll want to send other kinds of messages (e.g.,// snapshots) on the applyCh; at that point you can add fields to// ApplyMsg, but set CommandValid to false for these other uses.//type ApplyMsg struct { CommandValid bool Command interface{} CommandIndex int}//// A Go object implementing a single Raft peer.//type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer&#39;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer&#39;s persisted state me int // this peer&#39;s index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). // Look at the paper&#39;s Figure 2 for a description of what // state a Raft server must maintain. // (new). state int Time time.Time VoteCount int Timeout int // (new) end CurrentTerm int VoteFor int Log []LogEntry commitIndex int lastApplied int nextIndex []int matchIndex []int applyCh chan ApplyMsg}// return currentTerm and whether this server// believes it is the leader.func (rf *Raft) GetState() (int, bool) { var term int var isleader bool // Your code here (2A). rf.mu.Lock() defer rf.mu.Unlock() term = rf.CurrentTerm if rf.state == 0 { isleader = true } return term, isleader}//// save Raft&#39;s persistent state to stable storage,// where it can later be retrieved after a crash and restart.// see paper&#39;s Figure 2 for a description of what should be persistent.//func (rf *Raft) persist() { // Your code here (2C). // Example: w := new(bytes.Buffer) e := labgob.NewEncoder(w) e.Encode(rf.CurrentTerm) e.Encode(rf.VoteFor) e.Encode(rf.Log) data := w.Bytes() rf.persister.SaveRaftState(data)}//// restore previously persisted state.//func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) &amp;lt; 1 { // bootstrap without any state? return } // Your code here (2C). // Example: r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var term int var votefor int var log []LogEntry if d.Decode(&amp;amp;term) != nil || d.Decode(&amp;amp;votefor) != nil || d.Decode(&amp;amp;log) != nil { return } else { rf.CurrentTerm = term rf.VoteFor = votefor rf.Log = log }}// (2A) structure log entries.type LogEntry struct { Term int Command interface{} // still don&#39;t know about the command.}// (2A) AppendEntries structure.type AppendEntriesArgs struct { Term int Leaderindex int PrevLogIndex int PrevLogTerm int Entries []LogEntry LeaderCommit int}type AppendEntriesReply struct { Term int Success bool // (2C) new parameter. Xterm int Xindex int}// (2A) AppendEntries Handler.func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply){ rf.mu.Lock() defer rf.mu.Unlock() if args.Term &amp;lt; rf.CurrentTerm { reply.Term = rf.CurrentTerm reply.Success = false return } // (2B.) // (2B).Figure 2: AppendEntries RPC, implementation 2. something if args.Term &amp;gt; rf.CurrentTerm { rf.CurrentTerm = args.Term rf.persist() rf.convertToFollower() } if len(rf.Log) - 1 &amp;lt; args.PrevLogIndex { reply.Term = rf.CurrentTerm reply.Success = false reply.Xindex = len(rf.Log) reply.Xterm = -1 rf.Time = time.Now() return } if args.PrevLogIndex &amp;lt; len(rf.Log) &amp;amp;&amp;amp; rf.Log[args.PrevLogIndex].Term != args.PrevLogTerm { reply.Term = rf.CurrentTerm reply.Success = false reply.Xterm = rf.Log[args.PrevLogIndex].Term for i := 0;i &amp;lt; len(rf.Log);i++{ if rf.Log[i].Term == reply.Xterm { reply.Xindex = i break } } rf.Time = time.Now() return } // implementation 3 &amp;amp; implementation 4. delete existing entry if conflict happens index := -1 for i := 0;i &amp;lt; len(args.Entries);i++{ if len(rf.Log) &amp;lt; (args.PrevLogIndex + 2 + i) || rf.Log[args.PrevLogIndex + 1 + i].Term != args.Entries[i].Term{ index = i break } } if index != -1 { rf.Log = rf.Log[:index + args.PrevLogIndex + 1] for i := index;i &amp;lt; len(args.Entries);i++{ rf.Log = append(rf.Log,args.Entries[i]) } rf.persist() } // implementation 5. if args.LeaderCommit &amp;gt; rf.commitIndex { rf.Commit(min(args.LeaderCommit,len(rf.Log) - 1)) } reply.Term = rf.CurrentTerm reply.Success = true rf.Time = time.Now()}func (rf *Raft) sendAppendEntries(server int,args *AppendEntriesArgs, reply *AppendEntriesReply) bool { ok := rf.peers[server].Call(&quot;Raft.AppendEntries&quot;, args, reply) return ok}//// example RequestVote RPC arguments structure.// field names must start with capital letters!//type RequestVoteArgs struct { // Your data here (2A, 2B). Term int Candidateindex int LastLogIndex int LastLogTerm int}//// example RequestVote RPC reply structure.// field names must start with capital letters!//type RequestVoteReply struct { // Your data here (2A). Term int VoteGranted bool}//// example RequestVote RPC handler.//func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply){ // Your code here (2A, 2B). rf.mu.Lock() defer rf.mu.Unlock() if args.Term &amp;lt; rf.CurrentTerm { reply.Term = rf.CurrentTerm reply.VoteGranted = false; return } if args.Term &amp;gt; rf.CurrentTerm { rf.CurrentTerm = args.Term rf.persist() rf.convertToFollower() } if rf.VoteFor == -1 || rf.VoteFor == args.Candidateindex { // (2B. leader election.) if args.LastLogTerm &amp;gt; rf.Log[len(rf.Log) - 1].Term { rf.VoteFor = args.Candidateindex reply.VoteGranted = true reply.Term = rf.CurrentTerm rf.Time = time.Now() rf.persist() return } if (args.LastLogTerm == rf.Log[len(rf.Log) - 1].Term &amp;amp;&amp;amp; args.LastLogIndex &amp;gt;= len(rf.Log) - 1){ rf.VoteFor = args.Candidateindex reply.VoteGranted = true reply.Term = rf.CurrentTerm rf.Time = time.Now() rf.persist() return } } reply.VoteGranted = false reply.Term = rf.CurrentTerm}//// example code to send a RequestVote RPC to a server.// server is the index of the target server in rf.peers[].// expects RPC arguments in args.// fills in *reply with RPC reply, so caller should// pass &amp;amp;reply.// the types of the args and reply passed to Call() must be// the same as the types of the arguments declared in the// handler function (including whether they are pointers).//// The labrpc package simulates a lossy network, in which servers// may be unreachable, and in which requests and replies may be lost.// Call() sends a request and waits for a reply. If a reply arrives// within a timeout interval, Call() returns true; otherwise// Call() returns false. Thus Call() may not return for a while.// A false return can be caused by a dead server, a live server that// can&#39;t be reached, a lost request, or a lost reply.//// Call() is guaranteed to return (perhaps after a delay) *except* if the// handler function on the server sindexe does not return. Thus there// is no need to implement your own timeouts around Call().//// look at the comments in ../labrpc/labrpc.go for more details.//// if you&#39;re having trouble getting RPC to work, check that you&#39;ve// capitalized all field names in structs passed over RPC, and// that the caller passes the address of the reply struct with &amp;amp;, not// the struct itself.//func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) bool { ok := rf.peers[server].Call(&quot;Raft.RequestVote&quot;, args, reply) return ok}//// the service using Raft (e.g. a k/v server) wants to start// agreement on the next command to be appended to Raft&#39;s log. if this// server isn&#39;t the leader, returns false. otherwise start the// agreement and return immediately. there is no guarantee that this// command will ever be committed to the Raft log, since the leader// may fail or lose an election. even if the Raft instance has been killed,// this function should return gracefully.//// the first return value is the index that the command will appear at// if it&#39;s ever committed. the second return value is the current// term. the third return value is true if this server believes it is// the leader.//func (rf *Raft) Start(command interface{}) (int, int, bool) { index := -1 term := -1 isLeader := true // Your code here (2B). rf.mu.Lock() defer rf.mu.Unlock() if rf.state != Leader { isLeader = false return index, term, isLeader } // append command to rf.Log. new_log := LogEntry{} new_log.Term = rf.CurrentTerm new_log.Command = command rf.Log = append(rf.Log,new_log) // append new command to local log. rf.persist() index = len(rf.Log) - 1 term = rf.CurrentTerm rf.matchIndex[rf.me] = index rf.nextIndex[rf.me] = index + 1 return index, term, isLeader}//// the tester doesn&#39;t halt goroutines created by Raft after each test,// but it does call the Kill() method. your code can use killed() to// check whether Kill() has been called. the use of atomic avoindexs the// need for a lock.//// the issue is that long-running goroutines use memory and may chew// up CPU time, perhaps causing later tests to fail and generating// confusing debug output. any goroutine with a long-running loop// should call killed() to check whether it should stop.//func (rf *Raft) Kill() { atomic.StoreInt32(&amp;amp;rf.dead, 1) // Your code here, if desired. for { if rf.killed() { break } time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond) }}func (rf *Raft) killed() bool { z := atomic.LoadInt32(&amp;amp;rf.dead) return z == 1}func (rf *Raft) convertToLeader() { // (2B.) // initialization. (nextIndex and matchIndex.) rf.nextIndex = make([]int,len(rf.peers)) rf.matchIndex = make([]int,len(rf.peers)) for i:= 0;i &amp;lt; len(rf.peers);i++{ rf.nextIndex[i] = len(rf.Log) rf.matchIndex[i] = 0 } rf.state = Leader go rf.heartbeat() DPrintf(&quot;%d, become a leader&quot;,rf.me)}func (rf *Raft) converttoCandidate(){ rf.CurrentTerm++ rf.VoteFor = rf.me rf.state = Candidate rf.VoteCount = 1; rf.persist() // DPrintf(&quot;%d, become a Candidate&quot;,rf.me) go rf.getVote()}// send to other server except the Candidate server.func (rf *Raft) getVote(){ rf.CurrentTerm++ rf.Timeout = rand.Intn(200) + 400 rf.Time = time.Now() // DPrintf(&quot;%d, get vote&quot;,rf.me) rf.VoteCount = 1 // (2B. log replication) rf.persist() for i := 0;i &amp;lt; len(rf.peers);i++{ if i != rf.me { go func(index int){ rf.mu.Lock() args := RequestVoteArgs{ Term: rf.CurrentTerm, Candidateindex : rf.me, LastLogIndex: len(rf.Log) - 1, LastLogTerm: rf.Log[len(rf.Log) - 1].Term, } rf.mu.Unlock() reply := RequestVoteReply{} ok := rf.sendRequestVote(index,&amp;amp;args,&amp;amp;reply) if !ok { return } rf.mu.Lock() defer rf.mu.Unlock() if rf.state != Candidate { return } DPrintf(&quot;%d, get vote from: %d&quot;,rf.me,index) if reply.VoteGranted { rf.VoteCount++ } if rf.VoteCount &amp;gt; len(rf.peers) / 2{ rf.convertToLeader() return } }(i) } }}func (rf *Raft) convertToFollower(){ rf.state = Follower rf.VoteFor = -1 rf.persist() DPrintf(&quot;%d, convert to follower&quot;,rf.me)}// heartbeat. empty AppendEntries to keep authority.func (rf *Raft) heartbeat(){ for i := 0;i &amp;lt; len(rf.peers);i++{ if i != rf.me{ go func(index int) { rf.mu.Lock() if rf.state != Leader { rf.mu.Unlock() return } // DPrintf(&quot;%d, rf.nextIndex:%d, len:%d&quot;,rf.me,rf.nextIndex[index],len(rf.Log)) if len(rf.Log) &amp;lt; rf.nextIndex[index]{ DPrintf(&quot;%d, len: %d,rf.nextIndex[%d]: %d&quot;,rf.me,len(rf.Log),index,rf.nextIndex[index]) } prevLogIndex := rf.nextIndex[index] - 1 entries := make([]LogEntry, len(rf.Log[rf.nextIndex[index]:])) copy(entries, rf.Log[(prevLogIndex+1):]) args := AppendEntriesArgs{ Term : rf.CurrentTerm, Leaderindex : rf.me, PrevLogIndex :prevLogIndex, PrevLogTerm : rf.Log[prevLogIndex].Term, // Entries : rf.Log[PrevLogIndex + 1:], Entries: entries, LeaderCommit : rf.commitIndex, } if rf.state != Leader { rf.mu.Unlock() return } rf.mu.Unlock() reply := AppendEntriesReply{} ok := rf.sendAppendEntries(index,&amp;amp;args,&amp;amp;reply) if !ok { return } rf.mu.Lock() defer rf.mu.Unlock() if reply.Success { rf.matchIndex[index] = args.PrevLogIndex + len(args.Entries) rf.nextIndex[index] = rf.matchIndex[index] + 1 // check if replicated on majority of servers. for N := len(rf.Log) - 1; N &amp;gt; rf.commitIndex;N--{ num := 0 for j:= 0;j &amp;lt; len(rf.peers);j++{ if rf.matchIndex[j] &amp;gt;= N{ num++; } if num &amp;gt; len(rf.peers) / 2 { rf.Commit(N) break } } } } else { if reply.Term &amp;gt; rf.CurrentTerm { rf.CurrentTerm = reply.Term rf.persist() rf.convertToFollower() } else { // (2C.) if reply.Xindex &amp;gt; 0 { firstConflict := reply.Xindex if reply.Xterm != -1 { // not missing logs for i := 0; i &amp;lt; len(rf.Log); i++ { if rf.Log[i].Term != reply.Xterm { continue } for i &amp;lt; len(rf.Log) &amp;amp;&amp;amp; rf.Log[i].Term == reply.Xterm { i++ // the last conflict log&#39;s next index } firstConflict = i break } } rf.nextIndex[index] = firstConflict // next sync, send conflicted logs to the follower } } } }(i) } }}func (rf *Raft) Commit(N int){ rf.commitIndex = N for i := rf.lastApplied + 1;i &amp;lt;= rf.commitIndex;i++{ msg := ApplyMsg{ CommandValid: true, Command : rf.Log[i].Command, CommandIndex : i, } rf.lastApplied = i rf.applyCh &amp;lt;- msg }}func (rf *Raft) LeaderElection(){ for { rf.mu.Lock() DPrintf(&quot;%d, len: %d&quot;,rf.me,len(rf.Log)) switch rf.state { case Follower: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout{ rf.converttoCandidate() } rf.mu.Unlock() case Candidate: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout { rf.getVote() } rf.mu.Unlock() case Leader: rf.heartbeat() rf.mu.Unlock() } time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond) }}//// the service or tester wants to create a Raft server. the ports// of all the Raft servers (including this one) are in peers[]. this// server&#39;s port is peers[me]. all the servers&#39; peers[] arrays// have the same order. persister is a place for this server to// save its persistent state, and also initially holds the most// recent saved state, if any. applyCh is a channel on which the// tester or service expects Raft to send ApplyMsg messages.// Make() must return quickly, so it should start goroutines// for any long-running work.//func Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { rf := &amp;amp;Raft{} rf.peers = peers rf.persister = persister rf.me = me // Your initialization code here (2A, 2B, 2C). // (2A). Start. // rand.Seed(time.Now().Unix()) // rf.state = Follower rf.CurrentTerm = 0 rf.VoteFor = -1 rf.Time = time.Now() rf.Timeout = rand.Intn(200) + 400 // 2B initialization. // first_log := LogEntry{Term : -1} // rf.Log = append(rf.Log,first_log) rf.applyCh = applyCh // (2A) end // initialize from state persisted before a crash rf.readPersist(persister.ReadRaftState()) if len(rf.Log) == 0{ first_log := LogEntry{Term : -1} rf.Log = append(rf.Log,first_log) } // background goroutine. go rf.LeaderElection() return rf}" }, { "title": "6.824 Lab2C", "url": "/posts/6824-lab2C/", "categories": "Distributed System", "tags": "Distributed System, Raft", "date": "2020-12-28 21:24:10 +0800", "snippet": "Summary.I am a new programmer in Go languare. I spent almost 1 month implementing this lab2. Actually, I spent most of the time on debugging, just because I do not know much about Go. For example, I use rf.mu.Lock many times everywhere in raft.go. it is easier to go wrong because every goroutine can come to work when one Lock is release. This can result in some situations that a parameter change to another one value before you read it. when I am implementing lab2B, there is something wrong with lab2A code, and when I implement lab2C, there is something wrong with lab2B. LOL. My suggestion is if you struggle in some place and cannot find solution on the Internet, you can try detect the goroutine, or you can use sync.Cond in the code.Result:Code:func (rf *Raft) persist() { // Your code here (2C). // Example: w := new(bytes.Buffer) e := labgob.NewEncoder(w) e.Encode(rf.CurrentTerm) e.Encode(rf.VoteFor) e.Encode(rf.Log) data := w.Bytes() rf.persister.SaveRaftState(data)}func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) &amp;lt; 1 { // bootstrap without any state? return } // Your code here (2C). // Example: r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var term int var votefor int var log []LogEntry if d.Decode(&amp;amp;term) != nil || d.Decode(&amp;amp;votefor) != nil || d.Decode(&amp;amp;log) != nil { return } else { rf.CurrentTerm = term rf.VoteFor = votefor rf.Log = log }}" }, { "title": "6.824 Lab2B", "url": "/posts/6824-lab2B/", "categories": "Distributed System", "tags": "Distributed System, Raft", "date": "2020-12-26 01:21:10 +0800", "snippet": "Preparation. Raft paper Raft ExplanationSome details about Raft Explanation.(the link above)Actually, I know the big picture of the Raft consenus algorithm from the link above. Then I implement my code following the graph and I think there is something needed to be detailed here. When state is changed from Leader to Follower, there is no need to make the nextIndex and matchIndex empty. Although you can replicate majority of entries on servers, you still should implement the applyCh, because the tester know the committed log entries from that channel.Some Advices. Figure 2 is extremely precise, and every single statement it makes should be treated, in specification terms, as MUST, not as SHOULD. Even though you’ve passed Lab2A for many times by go test -run 2A, you should pass go test -race -run 2A before you move on to lab2B. When you have some problems about some parts in lab2B, maybe your code about log replication is fine and there is something wrong in Leader election(lab2A). Please look though all the code again. When you are debugging your raft code, you can use DPrintf everywhere, not just raft.go. For my code, DPrintf is used in test_test.go to help find where the bug is.Some Terminologies.Committed: A log entry is committed once the leader that created the entry has replicated it on a majority of the servers (e.g., entry 7 in Figure 6). This also commits all preceding entries in the leader’s log, including entries created by previous leaders.up-to-date: Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.Explanation:Start(): create a new log entry and append it to local Log[]. Log replication is through AppendEntries.commitIndex: It is not equal to len(rf.log). It is determined by highest log entry on majority of servers.lastApplied: Commit log entry and put it to applyCh channel.nextIndex: Although some sources said that nextIndex could be different, I just initialize it to the same as len(log), it could be changed.matchIndex: if executing successfully, it is equal to nextIndex - 1.prevLogIndex: equal to nextIndex - 1.entries[]: it is not all the log entries in the local log. it represents Log Entries which needed to be replicated on Follower. This field could be different when sending AppendEntrie to different servers.lastLogIndex: Candidate: len(rf.log).ResultCode:Advice: you should implement your own code before looking over my code.package raft//// this is an outline of the API that raft must expose to// the service (or tester). see comments below for// each of these functions for more details.//// rf = Make(...)// create a new Raft server.// rf.Start(command interface{}) (index, term, isleader)// start agreement on a new log entry// rf.GetState() (term, isLeader)// ask a Raft for its current term, and whether it thinks it is leader// ApplyMsg// each time a new entry is committed to the log, each Raft peer// should send an ApplyMsg to the service (or tester)// in the same server.//import &quot;sync&quot;import &quot;sync/atomic&quot;import &quot;../labrpc&quot;// new import.import &quot;time&quot;import &quot;math/rand&quot;// import &quot;bytes&quot;// import &quot;../labgob&quot;s// 3 states for servers.const Leader = 0const Candidate = 1const Follower = 2//// as each Raft peer becomes aware that Successive log entries are// committed, the peer should send an ApplyMsg to the service (or// tester) on the same server, via the applyCh passed to Make(). set// CommandValid to true to indicate that the ApplyMsg contains a newly// committed log entry.//// in Lab 3 you&#39;ll want to send other kinds of messages (e.g.,// snapshots) on the applyCh; at that point you can add fields to// ApplyMsg, but set CommandValid to false for these other uses.//type ApplyMsg struct { CommandValid bool Command interface{} CommandIndex int}//// A Go object implementing a single Raft peer.//type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer&#39;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer&#39;s persisted state me int // this peer&#39;s index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). // Look at the paper&#39;s Figure 2 for a description of what // state a Raft server must maintain. // (new). state int Time time.Time VoteCount int Timeout int // (new) end currentTerm int VoteFor int log []LogEntry commitIndex int lastApplied int nextIndex []int matchIndex []int applyCh chan ApplyMsg}// return currentTerm and whether this server// believes it is the leader.func (rf *Raft) GetState() (int, bool) { var term int var isleader bool // Your code here (2A). rf.mu.Lock() defer rf.mu.Unlock() term = rf.currentTerm if rf.state == 0 { isleader = true } return term, isleader}//// save Raft&#39;s persistent state to stable storage,// where it can later be retrieved after a crash and restart.// see paper&#39;s Figure 2 for a description of what should be persistent.//func (rf *Raft) persist() { // Your code here (2C). // Example: // w := new(bytes.Buffer) // e := labgob.NewEncoder(w) // e.Encode(rf.xxx) // e.Encode(rf.yyy) // data := w.Bytes() // rf.persister.SaveRaftState(data)}//// restore previously persisted state.//func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) &amp;lt; 1 { // bootstrap without any state? return } // Your code here (2C). // Example: // r := bytes.NewBuffer(data) // d := labgob.NewDecoder(r) // var xxx // var yyy // if d.Decode(&amp;amp;xxx) != nil || // d.Decode(&amp;amp;yyy) != nil { // error... // } else { // rf.xxx = xxx // rf.yyy = yyy // }}// (2A) structure log entries.type LogEntry struct { Term int Command interface{} // still don&#39;t know about the command.}// (2A) AppendEntries structure.type AppendEntriesArgs struct { Term int LeaderId int PrevLogIndex int PrevLogTerm int Entries []LogEntry LeaderCommit int}type AppendEntriesReply struct { Term int Success bool}// (2A) AppendEntries Handler.func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply){ rf.mu.Lock() defer rf.mu.Unlock() rf.Time = time.Now() if args.Term &amp;lt; rf.currentTerm { reply.Term = rf.currentTerm reply.Success = false return } // (2B.) // (2B).Figure 2: AppendEntries RPC, implementation 2. something if args.Term &amp;gt; rf.currentTerm { rf.currentTerm = args.Term rf.convertToFollower() } if len(rf.log) - 1 &amp;lt; args.PrevLogIndex { reply.Term = rf.currentTerm reply.Success = false return } if args.PrevLogIndex &amp;lt; len(rf.log) &amp;amp;&amp;amp; rf.log[args.PrevLogIndex].Term != args.PrevLogTerm { reply.Term = rf.currentTerm reply.Success = false return } // implementation 3 &amp;amp; implementation 4. delete existing entry if conflict happens index := -1 for i := 0;i &amp;lt; len(args.Entries);i++{ if len(rf.log) &amp;lt; (args.PrevLogIndex + 2 + i) || rf.log[args.PrevLogIndex + 1 + i].Term != args.Entries[i].Term{ index = i break } } if index != -1 { rf.log = rf.log[:index + args.PrevLogIndex + 1] for i := index;i &amp;lt; len(args.Entries);i++{ rf.log = append(rf.log,args.Entries[i]) } } // implementation 5. if args.LeaderCommit &amp;gt; rf.commitIndex { rf.Commit(min(args.LeaderCommit,len(rf.log) - 1)) } reply.Term = rf.currentTerm reply.Success = true}func (rf *Raft) sendAppendEntries(server int,args *AppendEntriesArgs, reply *AppendEntriesReply) bool { ok := rf.peers[server].Call(&quot;Raft.AppendEntries&quot;, args, reply) return ok}//// example RequestVote RPC arguments structure.// field names must start with capital letters!//type RequestVoteArgs struct { // Your data here (2A, 2B). Term int CandidateId int LastLogIndex int LastLogTerm int}//// example RequestVote RPC reply structure.// field names must start with capital letters!//type RequestVoteReply struct { // Your data here (2A). Term int VoteGranted bool}//// example RequestVote RPC handler.//func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply){ // Your code here (2A, 2B). rf.mu.Lock() defer rf.mu.Unlock() if args.Term &amp;lt; rf.currentTerm { reply.Term = rf.currentTerm reply.VoteGranted = false; return } if args.Term &amp;gt; rf.currentTerm { rf.currentTerm = args.Term rf.convertToFollower() } if rf.VoteFor == -1 || rf.VoteFor == args.CandidateId { // (2B. leader election.) if args.LastLogTerm &amp;gt; rf.log[len(rf.log) - 1].Term { rf.VoteFor = args.CandidateId reply.VoteGranted = true reply.Term = rf.currentTerm rf.Time = time.Now() return } if (args.LastLogTerm == rf.log[len(rf.log) - 1].Term &amp;amp;&amp;amp; args.LastLogIndex &amp;gt;= len(rf.log) - 1){ rf.VoteFor = args.CandidateId reply.VoteGranted = true reply.Term = rf.currentTerm rf.Time = time.Now() return } } reply.VoteGranted = false reply.Term = rf.currentTerm // rf.Time = time.Now()}//// example code to send a RequestVote RPC to a server.// server is the index of the target server in rf.peers[].// expects RPC arguments in args.// fills in *reply with RPC reply, so caller should// pass &amp;amp;reply.// the types of the args and reply passed to Call() must be// the same as the types of the arguments declared in the// handler function (including whether they are pointers).//// The labrpc package simulates a lossy network, in which servers// may be unreachable, and in which requests and replies may be lost.// Call() sends a request and waits for a reply. If a reply arrives// within a timeout interval, Call() returns true; otherwise// Call() returns false. Thus Call() may not return for a while.// A false return can be caused by a dead server, a live server that// can&#39;t be reached, a lost request, or a lost reply.//// Call() is guaranteed to return (perhaps after a delay) *except* if the// handler function on the server side does not return. Thus there// is no need to implement your own timeouts around Call().//// look at the comments in ../labrpc/labrpc.go for more details.//// if you&#39;re having trouble getting RPC to work, check that you&#39;ve// capitalized all field names in structs passed over RPC, and// that the caller passes the address of the reply struct with &amp;amp;, not// the struct itself.//func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) bool { ok := rf.peers[server].Call(&quot;Raft.RequestVote&quot;, args, reply) return ok}//// the service using Raft (e.g. a k/v server) wants to start// agreement on the next command to be appended to Raft&#39;s log. if this// server isn&#39;t the leader, returns false. otherwise start the// agreement and return immediately. there is no guarantee that this// command will ever be committed to the Raft log, since the leader// may fail or lose an election. even if the Raft instance has been killed,// this function should return gracefully.//// the first return value is the index that the command will appear at// if it&#39;s ever committed. the second return value is the current// term. the third return value is true if this server believes it is// the leader.//func (rf *Raft) Start(command interface{}) (int, int, bool) { index := -1 term := -1 isLeader := true // Your code here (2B). rf.mu.Lock() defer rf.mu.Unlock() if rf.state != Leader { isLeader = false return index, term, isLeader } // append command to rf.log. new_log := LogEntry{} new_log.Term = rf.currentTerm new_log.Command = command rf.log = append(rf.log,new_log) // append new command to local log. index = len(rf.log) - 1 term = rf.currentTerm rf.matchIndex[rf.me] = index rf.nextIndex[rf.me] = index + 1 for i:= 0;i &amp;lt; len(rf.peers);i++{ rf.nextIndex[i] = rf.nextIndex[rf.me] } return index, term, isLeader}//// the tester doesn&#39;t halt goroutines created by Raft after each test,// but it does call the Kill() method. your code can use killed() to// check whether Kill() has been called. the use of atomic avoids the// need for a lock.//// the issue is that long-running goroutines use memory and may chew// up CPU time, perhaps causing later tests to fail and generating// confusing debug output. any goroutine with a long-running loop// should call killed() to check whether it should stop.//func (rf *Raft) Kill() { atomic.StoreInt32(&amp;amp;rf.dead, 1) // Your code here, if desired. for { if rf.killed() { break } time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond) }}func (rf *Raft) killed() bool { z := atomic.LoadInt32(&amp;amp;rf.dead) return z == 1}func (rf *Raft) convertToLeader() { rf.state = Leader rf.Time = time.Now() // (2B.) // initialization. (nextIndex and matchIndex.) rf.nextIndex = make([]int,len(rf.peers)) rf.matchIndex = make([]int,len(rf.peers)) rf.nextIndex[rf.me] = len(rf.log) for i:= 0;i &amp;lt; len(rf.peers);i++{ rf.nextIndex[i] = rf.nextIndex[rf.me] } DPrintf(&quot;%d, become a leader&quot;,rf.me)}func (rf *Raft) convertToCandidate(){ rf.currentTerm++ rf.VoteFor = rf.me rf.Time = time.Now() rf.state = Candidate rf.VoteCount = 1; go rf.getVote() DPrintf(&quot;%d, become a candidate&quot;,rf.me)}// send to other server except the candidate server.func (rf *Raft) getVote(){ rf.mu.Lock() args := RequestVoteArgs{ Term: rf.currentTerm, CandidateId : rf.me, LastLogIndex: len(rf.log) - 1, LastLogTerm: rf.log[len(rf.log) - 1].Term, } // (2B. log replication) rf.mu.Unlock() for i := 0;i &amp;lt; len(rf.peers);i++{ if i != rf.me { go func(index int){ reply := RequestVoteReply{} ok := rf.sendRequestVote(index,&amp;amp;args,&amp;amp;reply) if !ok { return } rf.mu.Lock() if reply.VoteGranted { rf.VoteCount++ } if rf.VoteCount &amp;gt; len(rf.peers) / 2{ rf.convertToLeader() } rf.mu.Unlock() }(i) } }}func (rf *Raft) convertToFollower(){ rf.state = Follower rf.VoteFor = -1 // (2B.) also need a way to delete nextIndex and matchIndex. // my code: // rf.nextIndex = nil // rf.matchIndex = nil DPrintf(&quot;%d, become a follower&quot;,rf.me)}// heartbeat. empty AppendEntries to keep authority.func (rf *Raft) heartbeat(){ rf.Time = time.Now() // initialization args. if rf.state != Leader { return } for i := 0;i &amp;lt; len(rf.peers);i++{ if i != rf.me{ go func(index int) { rf.mu.Lock() if rf.state != Leader { rf.mu.Unlock() return } PrevLogIndex := rf.nextIndex[index] - 1 args := AppendEntriesArgs{ Term : rf.currentTerm, LeaderId : rf.me, PrevLogIndex :PrevLogIndex, PrevLogTerm : rf.log[PrevLogIndex].Term, Entries : rf.log[PrevLogIndex + 1:], LeaderCommit : rf.commitIndex, } rf.mu.Unlock() if rf.state != Leader { return } reply := AppendEntriesReply{} ok := rf.sendAppendEntries(index,&amp;amp;args,&amp;amp;reply) if !ok { return } rf.mu.Lock() if reply.Success { rf.matchIndex[index] = args.PrevLogIndex + len(args.Entries) rf.nextIndex[index] = rf.matchIndex[index] + 1 // check if replicated on majority of servers. for N := len(rf.log) - 1; N &amp;gt; rf.commitIndex;N--{ num := 0 for j:= 0;j &amp;lt; len(rf.peers);j++{ if rf.matchIndex[j] &amp;gt;= N{ num++; } if num &amp;gt; len(rf.peers) / 2 { rf.Commit(N) break } } } } else { if reply.Term &amp;gt; rf.currentTerm { rf.currentTerm = reply.Term rf.convertToFollower() } else { if rf.nextIndex[index] &amp;gt; 1 { rf.nextIndex[index] = args.PrevLogIndex } } } rf.mu.Unlock() }(i) } }}func (rf *Raft) Commit(N int){ rf.commitIndex = N for i := rf.lastApplied + 1;i &amp;lt;= rf.commitIndex;i++{ msg := ApplyMsg{ CommandValid : true, Command : rf.log[i].Command, CommandIndex : i, } rf.lastApplied = i rf.applyCh &amp;lt;- msg }}func (rf *Raft) LeaderElection(){ for { rf.mu.Lock() DPrintf(&quot;%d, %v&quot;,rf.me,rf.log) // DPrintf(&quot;%d, state: %d&quot;,rf.me,rf.state) // DPrintf(&quot;%d, VoteFor: %d, state: %d&quot;,rf.me,rf.VoteFor,rf.state) switch rf.state { case Follower: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout{ rf.convertToCandidate() } rf.mu.Unlock() case Candidate: if int(time.Since(rf.Time) /time.Millisecond) &amp;gt; rf.Timeout { rf.currentTerm++ rf.Timeout = rand.Intn(150) + 150 go rf.getVote() } rf.mu.Unlock() case Leader: rf.heartbeat() go rf.mu.Unlock() } time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond) }}//// the service or tester wants to create a Raft server. the ports// of all the Raft servers (including this one) are in peers[]. this// server&#39;s port is peers[me]. all the servers&#39; peers[] arrays// have the same order. persister is a place for this server to// save its persistent state, and also initially holds the most// recent saved state, if any. applyCh is a channel on which the// tester or service expects Raft to send ApplyMsg messages.// Make() must return quickly, so it should start goroutines// for any long-running work.//func Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { rf := &amp;amp;Raft{} rf.peers = peers rf.persister = persister rf.me = me // Your initialization code here (2A, 2B, 2C). // (2A). Start. // rand.Seed(time.Now().Unix()) // rf.state = Follower rf.currentTerm = 0 rf.VoteFor = -1 rf.Time = time.Now() rf.Timeout = rand.Intn(150) + 150 // 2B initialization. first_log := LogEntry{Term : -1} rf.log = append(rf.log,first_log) rf.commitIndex = 0 rf.lastApplied = 0 rf.applyCh = applyCh // background goroutine. go rf.LeaderElection() // (2A) end // initialize from state persisted before a crash rf.readPersist(persister.ReadRaftState()) return rf}" }, { "title": "6.824 Lab2A", "url": "/posts/6824-lab2A/", "categories": "Distributed System", "tags": "Distributed System, Raft", "date": "2020-12-05 10:59:10 +0800", "snippet": "Preparation. Raft paperSummary.There is only one leader in Raft algorithm. Leader remains authority through sending AppendEntries to peers.if one peer doesn’t hear from the leader or Candidate, it starts election through sending RequestVote to other peers, if Candidate gain majority of Votes from peers and it would become leader and send AppendEntries to other peers.Some details. if there is already one leader in raft, and for some unpredicted reason, AppendEntries is failed and other peers does not receive that signal, based on Raft algorithm, the peer would become Candidate and start election. If that peer eventually becomes Leader and the old leader comes back to work. There is a conflict here. My thought is change the old leader to Follower again through rf.currentTerm. if more than one Followers change to Candidate at the same time, called split vote. Raft use random timeout for different servers. In this case, only one candidate could start election eventually through increasing currentTerm. test code with go test -run 2A. try it with more than 1 time. For me, I try it 10 times to guarantee the code is running successfully always.Hint: go test -run 2A. Done. focus on 3 things. RequestVote, Rules for servers and State related to leader election. FollowerTask: update time when receiving signal from Candidates or Leader. If not, becomes Candidates and start election. Candidatessend RequestVote to peers and gain Votes. LeaderSend AppendEntries to other peers. Remember to change ole Leader to Follower if there is more than one Leader. new to raft type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer&#39;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer&#39;s persisted state me int // this peer&#39;s index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). // Look at the paper&#39;s Figure 2 for a description of what // state a Raft server must maintain. // (new). state int Time time.Time VoteCount int Timeout int // (new) end currentTerm int VoteFor int log []LogEntry commitIndex int lastApplied int nextIndex []int matchIndex []int} RequestVote: type RequestVoteArgs struct { // Your data here (2A, 2B). Term int CandidateId int LastLogIndex int LastLogTerm int}type RequestVoteReply struct { // Your data here (2A). Term int VoteGranted bool}//func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply){ // Your code here (2A, 2B). rf.mu.Lock() rf.Time = time.Now() rf.mu.Unlock() if args.Term &amp;lt; rf.currentTerm { reply.VoteGranted = false; } if rf.VoteFor == -1 || rf.VoteFor == args.CandidateId { reply.VoteGranted = true } if args.Term &amp;gt; rf.currentTerm{ rf.mu.Lock() rf.currentTerm = args.Term rf.mu.Unlock() rf.convertToFollower() DPrintf(&quot;%d converts to follower&quot;,rf.me) }} AppendEntriestype AppendEntriesArgs struct { Term int LeaderId int PrevLogIndex int PrevLogTerm int Entries []LogEntry LeaderCommit int}type AppendEntriesReply struct { Term int Success bool}// (2A) AppendEntries Handler.func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply){ if args.Term &amp;lt; rf.currentTerm { reply.Success = false }else{ rf.convertToFollower() reply.Success = true } rf.Time = time.Now()} Timeout. rf.Timeout = rand.Intn(150) + 150 range[150,300] Update time. tf.Time = time.Now()Result" }, { "title": "6.824 Lab1 MapReduce", "url": "/posts/firstPost/", "categories": "Distributed System, MIT 6824", "tags": "Distributed System, MapReduce", "date": "2020-11-24 16:29:10 +0800", "snippet": "Preparation MapReduce Paper Lab1SummaryLab1 is to implement a Worker process and a Master process. Whole process: go build -buildmode=plugin ../mrapps/wc.go first build word-count plugin.go run mrmaster.go pg-*.txt runs a Master process and takes pg-*.txt as input files.go run mrworker.go wc.so runs a worker process. Jobs:Three files that need to be implemented.mr/worker.go Worker Process calls Map or Reduce functions when Master hands out tasks to it.mr/master.go check tasks are available, hands out tasks to worker process.mr/rpc.go Communication between Worker and Master.ImplementationSince I am not allowed to publish my code, I am going to put some hints here.master.gotype Master struct { // Your definitions here. mu sync.Mutex // Lock for Master mapTask []Task // Map tasks, pg-*.txt. reduceTask []Task // Reduce tasks. intermediate map[int][]string // intermediate files after Map functions finished, // but before Reduce functions begins. NReduce int State string // states End int} mapTask Holds all files which need to be mapped. Actually, files are pg-*.txt. reduceTask Similar to mapTask. in my code, the size of it is equal to nReduce. intermediateMatch mapTask to reduceTask. like a bridge connect between them. The size of it is M * N, M is the size of mapTask and N is equal to nReduce. State 4 states. “map”, “wait”, “reduce”, “finished”.rpc.gotype Task struct { Type string // &quot;UnAssigned, Assigned, Finished&quot; Id int Filename string // pg-*.txt State int // finished or not NReduce int Time time.Time // record the time. Inter []string // intermiediate files after Map function is finished.} State 3 states: “UnAssigned”, “Assigned”, “Finished”. Time Master should resend the task to Worker if worker fails for some reasons. Inter files need to be Reduced.worker.gofunc Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { // endless loop. for true { args := Task{} reply := Task{} Initialization(&amp;amp;args,&amp;amp;reply) call(&quot;Master.Handler&quot;,&amp;amp;args,&amp;amp;reply) CopyTask(&amp;amp;args,&amp;amp;reply) switch reply.Type { case &quot;Finished&quot;: return case &quot;Wait&quot;: time.Sleep(time.Second) continue case &quot;map&quot;: Execute_map(&amp;amp;args,mapf) call(&quot;Master.UpdateMaster&quot;,&amp;amp;args,&amp;amp;reply) case &quot;reduce&quot;: Execute_reduce(&amp;amp;args,reducef) call(&quot;Master.UpdateMaster&quot;,&amp;amp;args,&amp;amp;reply) } }}Result" }, { "title": "JOS kernel", "url": "/posts/6828/", "categories": "OS, MIT 6828", "tags": "OS, MIT 6828", "date": "2020-11-23 10:05:10 +0800", "snippet": " Link JOS kernel I’ve uploaded all the code about JOS kernel. Features.lab2 Mapped virtual address to physical address.lab3 Implemented a protected user-mode environment.lab4 Implemented preemptive multitasking among multiple simultaneously active user-mode environments.lab5 Flesh out kernel and library operating system enough to run a shell on the console." }, { "title": "Scrapy Example Flight", "url": "/posts/flight/", "categories": "Scrapy", "tags": "Scrapy", "date": "2020-08-23 16:29:10 +0800", "snippet": " Link Flight backgroundI am an international student who need to book a flight from US to China. However, all the tickets are sold out in a minute. I write a script to check the ticket is available or not. Finally, I booked a ticket at August 5th. " } ]
