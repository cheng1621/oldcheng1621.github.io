---
layout: post
title:  "gfs Note"
date: 2021-06-26 8:10:10 +0800
categories: [Distributed System, MIT 6824]
tags: [Distributed System, gfs]
pin : true
---

# why.
component failure is norm rather than exception.  
files are huge by traditional standards.  

# design overview. 
- suppose the component often fail.
- make optimation for multi-GB files.
- for random reads, batch and sort them to advance steadily through files rather than go back and forth.  
- concurrent appendance by multiple clients to a same file is also supported.  

# architecture. 
a `master` and several `chunkservers`.  
master maintains all the system metadata, including namespace,location ... etc.  
neither the client nor the chunkserver caches file data, since the file maybe too large to be cached.  

## Single master. 
process: client send packets consists of the `file name` and `chunk index` and the server responds with the locations of the chunk. Then client to one of the replicas for the data. 
`no more client-master interactions are needed until the information expired`.  

## Chunk Size.
usually 64 MB. 
advantages:
- reduce client-master interaction. Read and writes on the same file need one interaction.  
- a client is more likely to perform many operations on the same files, it could reduce the network overhead. 
- reduce the metadata which is stored in master.  

## Metadata. 
three metadata: `file namespace and chunk namespace`, `mapping file to chunk` and `locations of chunk's replicas.`  

### In-memory Data Structure.
advantages: easy to scan through master's state and implement the chunk garbage collection.  

### chunk location.
- easy to request the data at startup.
- if the chunk is moved or failed, there is no point to maintain all the information.  

### Operation Log.
replicate operation log on mutiple remote machines and respond to a client operation after flushing the OL to disk both locally and remotely.  

## Consistency Model. 
### guarantee by GFS.
some terminology: `defined`, `consistent`.  
GFS identifies failed chunkserver by checksumming.

### implication for applications. 
application mutates file by appending rather than overwriting. that means, a writer generates a file from begining to end. 

# system interaction.
master granted a `lease` to one of the replicas, named `primary`. primary define a mutation order. Sometimes primary could revoke the lease to disable the mutation on a file.  

```
1. client send requests to master for metadata of the chunk.  
2. client push data to replicas.
3. after the step 2, client send write request to primary.
4. primary forward write request to secondary replica.
5. secondary replies to primary.
6. primary replies to client.  
```

## data flow.
to avoid network latency, one server send the information to closest server, instead of `tree topology`.  

## atomic Record appends.
GFS only guarantee the data is written at least once as an atomic unit, which means that data could include duplicate records.  

## snapshot.  
`copy on write` 

# master operation.  
## namespace management and locking.
GFS does not have a per-directory data structure that lists all the files in directory, nor does it support aliases.  

## Replica Placement.  
## Creation, Re-replication, reblancing.  

## garbage collection. 
instead of reclaiming the delete object, the file changed to hidden object.  

## stale replica detection. 
chunk version number is needed. 

# fault tolerance and diagnosis. 
## high availability
`fast recovery`, `chunk replication` and `master replication`.  
if master is down, `shadow master` provides read-only access to the file system.  
## data integrity. 
checksumming. 